{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"guides/","title":"Guides","text":"<ul> <li>Querying Data</li> </ul>"},{"location":"guides/querying-data/","title":"Data Querying for Data Scientists","text":""},{"location":"guides/querying-data/#a-comprehensive-guide-of-using-pandas-sql-pyspark-and-polars-for-data-manipulation-techniques-with-practical-examples-and-visualisations","title":"A Comprehensive Guide of using Pandas, SQL, PySpark, and Polars for Data Manipulation Techniques, with Practical Examples and Visualisations","text":"<p>If you wanted to run the code yourself, you can download just that Jupyter notebook:</p> <p>Download- ALL Download- Pandas Download- SQL Download- PySpark Download- Polars</p> <p>Or you can follow along on this page...</p>"},{"location":"guides/querying-data/#introduction","title":"Introduction","text":"<p>Working as a Data Scientist or Data Engineer often involves querying data from various sources. There are many tools and libraries available to perform these tasks, each with its own strengths and weaknesses. Also, there are many different ways to achieve similar results, depending on the tool or library used. It's important to be familiar with these different methods to choose the best one for your specific use case.</p> <p>This article provides a comprehensive guide on how to query data using different tools and libraries, including Pandas, SQL, PySpark, and Polars. Each section will cover the setup, data creation, and various querying techniques such as filtering, grouping, joining, window functions, ranking, and sorting. The output will be identical across all tools, but the transformations will be implemented using the specific syntax and features of each library. Therefore allowing you to compare the different approaches and understand the nuances of each method.</p>"},{"location":"guides/querying-data/#overview-of-the-different-libraries","title":"Overview of the Different Libraries","text":"<p>Before we dive into the querying techniques, let's take a moment to understand the different libraries and tools we will be using in this article. Each library has its own strengths and weaknesses, and understanding these can help you choose the right tool for your specific use case.</p> <p>Throughout this article, you can easily switch between the different libraries by selecting the appropriate tab. Each section will provide the same functionality, but implemented using the specific syntax and features of each library.</p> PandasSQLPySparkPolars <p>Pandas is a powerful data manipulation library in Python that provides data structures and functions for working with structured data. It is widely used for data analysis and manipulation tasks.</p> <p>Historically, Pandas was one of the first libraries to provide a DataFrame structure, which is similar to a table in a relational database. It allows for easy data manipulation, filtering, grouping, and aggregation. Pandas is built on top of NumPy and provides a high-level interface for working with data. It is particularly well-suited for small to medium-sized datasets and is often used in Data Science and Machine Learning workflows.</p> <p>Pandas provides a rich set of functionalities for data manipulation, including filtering, grouping, joining, and window functions. It also integrates well with other libraries such as Matplotlib and Seaborn for data visualization, making it a popular choice among data scientists and analysts.</p> <p>While Pandas is both powerful and popular, it is important to note that it operates in-memory, which means that it may not be suitable for very large datasets that do not fit into memory. In such cases, other libraries like PySpark or Polars may be more appropriate.</p> <p>SQL (Structured Query Language) is a standard language for managing and manipulating relational databases. It is widely used for querying and modifying data in databases. SQL is a declarative language, meaning that you specify what you want to retrieve or manipulate without detailing how to do it. This makes SQL queries concise and expressive. SQL is particularly well-suited for working with large datasets and complex queries. It provides powerful features for filtering, grouping, joining, and aggregating data. SQL is the backbone of many database systems.</p> <p>SQL is actually a language (like Python is a language), not a library (like Pandas is a library), and it is used to interact with relational databases. The core of the SQL language is actually an ISO standard, which means that the basic syntax and functionality are consistent across different database systems. However, each database system may have its own extensions or variations of SQL, which can lead to differences in syntax and features. Each database system can be considered as variations (or dialects) of SQL, with their own specific features and optimizations and syntax enhancements.</p> <p>Some of the more popular SQL dialects include:</p> <ul> <li>SQLite</li> <li>PostgreSQL</li> <li>MySQL</li> <li>Spark SQL</li> <li>SQL Server (t-SQL)</li> <li>Oracle SQL (pl-SQL)</li> </ul> <p>PySpark is the Python API for Apache Spark, a distributed computing framework that allows for large-scale data processing. PySpark provides a high-level interface for working with Spark, making it easier to write distributed data processing applications in Python. It is particularly well-suited for big data processing and analytics.</p> <p>PySpark provides a DataFrame API similar to Pandas, but it is designed to work with large datasets that do not fit into memory. It allows for distributed data processing across a cluster of machines, making it suitable for big data applications. PySpark supports various data sources, including HDFS, S3, ADLS, and JDBC, and provides powerful features for filtering, grouping, joining, and aggregating data.</p> <p>While PySpark is a powerful tool for big data processing, it can be more complex to set up and use compared to Pandas. It requires a Spark cluster and may have a steeper learning curve for those unfamiliar with distributed computing concepts. However, it is an excellent choice for processing large datasets and performing complex data transformations.</p> <p>Polars is a fast DataFrame library for Python that is designed for high-performance data manipulation. It is built on top of Rust and provides a DataFrame API similar to Pandas, but with a focus on performance and memory efficiency. Polars is particularly well-suited for large datasets and complex queries.</p> <p>Polars supports lazy evaluation, which allows for optimizations in query execution. Polars also provides powerful features for filtering, grouping, joining, and aggregating data, making it a great choice for data analysis tasks.</p> <p>While Polars is a relatively new library compared to Pandas, it has gained popularity for its performance and ease of use. It is designed to be a drop-in replacement for Pandas, allowing users to leverage its performance benefits without significant changes to their existing code. It is particularly useful for data scientists and analysts who need to work with large datasets and require fast data manipulation capabilities. The setup is simple and straightforward, similar to Pandas, and less complex than PySpark. It is a great choice for data analysis tasks that require high performance and memory efficiency.</p>"},{"location":"guides/querying-data/#setup","title":"Setup","text":"<p>Before we start querying data, we need to set up our environment. This includes importing the necessary libraries, creating sample data, and defining constants that will be used throughout the article. The following sections will guide you through this setup process. The code for this article is also available on GitHub: querying-data.</p> PandasSQLPySparkPolars Setup<pre><code># StdLib Imports\nfrom typing import Any\n\n# Third Party Imports\nimport numpy as np\nimport pandas as pd\nfrom plotly import express as px, graph_objects as go, io as pio\n\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Determine the number of records to generate\nn_records = 100\n\n# Set default Plotly template\npio.templates.default = \"simple_white+gridon\"\n\n# Set Pandas display options\npd.set_option(\"display.max_columns\", None)\n</code></pre> Setup<pre><code># StdLib Imports\nimport sqlite3\nfrom typing import Any\n\n# Third Party Imports\nimport numpy as np\nimport pandas as pd\nfrom plotly import express as px, graph_objects as go, io as pio\n\n\n# Set default Plotly template\npio.templates.default = \"simple_white+gridon\"\n\n# Set Pandas display options\npd.set_option(\"display.max_columns\", None)\n</code></pre> Setup<pre><code># StdLib Imports\nfrom typing import Any\n\n# Third Party Imports\nimport numpy as np\nfrom plotly import express as px, graph_objects as go, io as pio\nfrom pyspark.sql import (\n    DataFrame as psDataFrame,\n    SparkSession,\n    Window,\n    functions as F,\n    types as T,\n)\n\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Determine the number of records to generate\nn_records = 100\n\n# Set default Plotly template\npio.templates.default = \"simple_white+gridon\"\n</code></pre> Setup<pre><code># StdLib Imports\nfrom typing import Any\n\n# Third Party Imports\nimport numpy as np\nimport polars as pl\nfrom plotly import express as px, graph_objects as go, io as pio\n\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Determine the number of records to generate\nn_records = 100\n\n# Set default Plotly template\npio.templates.default = \"simple_white+gridon\"\n\n# Set Polars display options\npl.Config.set_tbl_cols(-1)\n</code></pre> <p>Once the setup is complete, we can proceed to create our sample data. This data will be used for querying and will be consistent across all libraries. All tables will be created from scratch with randomly generated data to simulate a real-world scenario. This is to ensure that the examples are self-contained and can be run without any external dependencies, and also there is no issues about data privacy or security.</p> <p>For the below data creation steps, we will be defining the tables using Python dictionaries. Each dictionary will represent a table, with keys as column names and values as lists of data. We will then convert these dictionaries into DataFrames or equivalent structures in each library.</p> <p>First, we will create a sales fact table. This table will contain information about sales transactions, including the date, customer ID, product ID, category, sales amount, and quantity sold.</p> Create Sales Fact Data<pre><code>sales_data: dict[str, Any] = {\n    \"date\": pd.date_range(start=\"2023-01-01\", periods=n_records, freq=\"D\"),\n    \"customer_id\": np.random.randint(1, 100, n_records),\n    \"product_id\": np.random.randint(1, 50, n_records),\n    \"category\": np.random.choice([\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Home\"], n_records),\n    \"sales_amount\": np.random.uniform(10, 1000, n_records).round(2),\n    \"quantity\": np.random.randint(1, 10, n_records),\n}\n</code></pre> <p>Next, we will create a product dimension table. This table will contain information about products, including the product ID, name, price, category, and supplier ID.</p> Create Product Dimension Data<pre><code>product_data: dict[str, Any] = {\n    \"product_id\": np.arange(1, 51),\n    \"product_name\": [f\"Product {i}\" for i in range(1, 51)],\n    \"price\": np.random.uniform(10, 500, 50).round(2),\n    \"category\": np.random.choice([\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Home\"], 50),\n    \"supplier_id\": np.random.randint(1, 10, 50),\n}\n</code></pre> <p>Finally, we will create a customer dimension table. This table will contain information about customers, including the customer ID, name, city, state, and segment.</p> Create Customer Dimension Data<pre><code>customer_data: dict[str, Any] = {\n    \"customer_id\": np.arange(1, 101),\n    \"customer_name\": [f\"Customer {i}\" for i in range(1, 101)],\n    \"city\": np.random.choice([\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"], 100),\n    \"state\": np.random.choice([\"NY\", \"CA\", \"IL\", \"TX\", \"AZ\"], 100),\n    \"segment\": np.random.choice([\"Consumer\", \"Corporate\", \"Home Office\"], 100),\n}\n</code></pre> <p>Now that we have our sample data created, we can proceed to the querying section. Each of the following sections will demonstrate how to perform similar operations using the different libraries and methods, allowing you to compare and contrast their capabilities.</p>"},{"location":"guides/querying-data/#create-the-dataframes","title":"Create the DataFrames","text":"PandasSQLPySparkPolars <p>To create the dataframes in Pandas, we will use the data we generated earlier. We will parse the dictionaries into Pandas DataFrames, which will allow us to perform various data manipulation tasks.</p> Create DataFrames<pre><code>df_sales_pd: pd.DataFrame = pd.DataFrame(sales_data)\ndf_product_pd: pd.DataFrame = pd.DataFrame(product_data)\ndf_customer_pd: pd.DataFrame = pd.DataFrame(customer_data)\n</code></pre> <p>Once the data is created, we can check that it has been loaded correctly by displaying the first few rows of each DataFrame. To do this, we will use the <code>.head()</code> method to display the first 5 rows of each DataFrame, and then parse to the <code>print()</code> function to display the DataFrame in a readable format.</p> Check Sales DataFrame<pre><code>print(f\"Sales DataFrame: {len(df_sales_pd)}\")\nprint(df_sales_pd.head(5))\nprint(df_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Sales DataFrame: 100\n</code></pre> <pre><code>        date  customer_id  product_id     category  sales_amount  quantity\n0 2023-01-01           52          45         Food        490.76         7\n1 2023-01-02           93          41  Electronics        453.94         5\n2 2023-01-03           15          29         Home        994.51         5\n3 2023-01-04           72          15  Electronics        184.17         7\n4 2023-01-05           61          45         Food         27.89         9\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-01 00:00:00 52 45 Food 490.76 7 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 2 2023-01-03 00:00:00 15 29 Home 994.51 5 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Check Product DataFrame<pre><code>print(f\"Product DataFrame: {len(df_product_pd)}\")\nprint(df_product_pd.head(5))\nprint(df_product_pd.head(5).to_markdown())\n</code></pre> <pre><code>Product DataFrame: 50\n</code></pre> <pre><code>   product_id product_name   price  category  supplier_id\n0           1    Product 1  257.57      Food            8\n1           2    Product 2  414.96  Clothing            5\n2           3    Product 3  166.82  Clothing            8\n3           4    Product 4  448.81      Food            4\n4           5    Product 5  200.71      Food            8\n</code></pre> product_id product_name price category supplier_id 0 1 Product 1 257.57 Food 8 1 2 Product 2 414.96 Clothing 5 2 3 Product 3 166.82 Clothing 8 3 4 Product 4 448.81 Food 4 4 5 Product 5 200.71 Food 8 Check Customer DataFrame<pre><code>print(f\"Customer DataFrame: {len(df_customer_pd)}\")\nprint(df_customer_pd.head(5))\nprint(df_customer_pd.head(5).to_markdown())\n</code></pre> <pre><code>Customer DataFrame: 100\n</code></pre> <pre><code>   customer_id customer_name         city state      segment\n0            1    Customer 1      Phoenix    NY    Corporate\n1            2    Customer 2      Phoenix    CA  Home Office\n2            3    Customer 3      Phoenix    NY  Home Office\n3            4    Customer 4  Los Angeles    NY     Consumer\n4            5    Customer 5  Los Angeles    IL  Home Office\n</code></pre> customer_id customer_name city state segment 0 1 Customer 1 Phoenix NY Corporate 1 2 Customer 2 Phoenix CA Home Office 2 3 Customer 3 Phoenix NY Home Office 3 4 Customer 4 Los Angeles NY Consumer 4 5 Customer 5 Los Angeles IL Home Office <p>To create the dataframes in SQL, we will use the data we generated earlier. Firstly, we need to create the SQLite database. This will be an in-memory database for demonstration purposes, but in a real-world scenario, you would typically connect to a persistent (on-disk) database. To do this, we will use the <code>sqlite3</code> library to create a connection to the database, which we define with the <code>:memory:</code> parameter on the <code>.connect()</code> function. The result is to create a temporary database that exists only during the lifetime of the connection.</p> <p>Next, we will then parse the dictionaries into Pandas DataFrames, which will then be loaded into an SQLite database. This allows us to perform various data manipulation tasks using SQL queries.</p> Create DataFrames<pre><code># Creates SQLite database and tables\nconn: sqlite3.Connection = sqlite3.connect(\":memory:\")\npd.DataFrame(sales_data).to_sql(\"sales\", conn, index=False, if_exists=\"replace\")\npd.DataFrame(product_data).to_sql(\"product\", conn, index=False, if_exists=\"replace\")\npd.DataFrame(customer_data).to_sql(\"customer\", conn, index=False, if_exists=\"replace\")\n</code></pre> <p>Once the data is created, we can check that it has been loaded correctly by displaying the first few rows of each DataFrame. To do this, we will use the <code>pd.read_sql()</code> function to execute SQL queries and retrieve the data from the database. We will then parse the results to the <code>print()</code> function to display the DataFrame in a readable format.</p> Check Sales DataFrame<pre><code>print(f\"Sales Table: {len(pd.read_sql('SELECT * FROM sales', conn))}\")\nprint(pd.read_sql(\"SELECT * FROM sales LIMIT 5\", conn))\nprint(pd.read_sql(\"SELECT * FROM sales LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Sales Table: 100\n</code></pre> <pre><code>                  date  customer_id  product_id     category  sales_amount  quantity\n0  2023-01-01 00:00:00           52          45         Food        490.76         7\n1  2023-01-02 00:00:00           93          41  Electronics        453.94         5\n2  2023-01-03 00:00:00           15          29         Home        994.51         5\n3  2023-01-04 00:00:00           72          15  Electronics        184.17         7\n4  2023-01-05 00:00:00           61          45         Food         27.89         9\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-01 00:00:00 52 45 Food 490.76 7 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 2 2023-01-03 00:00:00 15 29 Home 994.51 5 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Check Product DataFrame<pre><code>print(f\"Product Table: {len(pd.read_sql('SELECT * FROM product', conn))}\")\nprint(pd.read_sql(\"SELECT * FROM product LIMIT 5\", conn))\nprint(pd.read_sql(\"SELECT * FROM product LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Product Table: 50\n</code></pre> <pre><code>   product_id product_name   price  category  supplier_id\n0           1    Product 1  257.57      Food            8\n1           2    Product 2  414.96  Clothing            5\n2           3    Product 3  166.82  Clothing            8\n3           4    Product 4  448.81      Food            4\n4           5    Product 5  200.71      Food            8\n</code></pre> product_id product_name price category supplier_id 0 1 Product 1 257.57 Food 8 1 2 Product 2 414.96 Clothing 5 2 3 Product 3 166.82 Clothing 8 3 4 Product 4 448.81 Food 4 4 5 Product 5 200.71 Food 8 Check Customer DataFrame<pre><code>print(f\"Customer Table: {len(pd.read_sql('SELECT * FROM customer', conn))}\")\nprint(pd.read_sql(\"SELECT * FROM customer LIMIT 5\", conn))\nprint(pd.read_sql(\"SELECT * FROM customer LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Customer Table: 100\n</code></pre> <pre><code>   customer_id customer_name         city state      segment\n0            1    Customer 1      Phoenix    NY    Corporate\n1            2    Customer 2      Phoenix    CA  Home Office\n2            3    Customer 3      Phoenix    NY  Home Office\n3            4    Customer 4  Los Angeles    NY     Consumer\n4            5    Customer 5  Los Angeles    IL  Home Office\n</code></pre> customer_id customer_name city state segment 0 1 Customer 1 Phoenix NY Corporate 1 2 Customer 2 Phoenix CA Home Office 2 3 Customer 3 Phoenix NY Home Office 3 4 Customer 4 Los Angeles NY Consumer 4 5 Customer 5 Los Angeles IL Home Office <p>Spark DataFrames are similar to Pandas DataFrames, but they are designed to work with large datasets that do not fit into memory. They can be distributed across a cluster of machines, allowing for parallel processing of data.</p> <p>To create the dataframes in PySpark, we will use the data we generated earlier. We will first create a Spark session, which is the entry point to using PySpark. Then, we will parse the dictionaries into PySpark DataFrames, which will allow us to perform various data manipulation tasks.</p> <p>The PySpark session is created using the <code>.builder</code> method on the <code>SparkSession</code> class, which allows us to configure the session with various options such as the application name. The <code>.getOrCreate()</code> method is used to either get an existing session or create a new one if it doesn't exist.</p> Create Spark Session<pre><code>spark: SparkSession = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()\n</code></pre> <p>Once the Spark session is created, we can create the DataFrames from the dictionaries. We will  use the <code>.createDataFrame()</code> method on the Spark session to convert the dictionaries into PySpark DataFrames. The <code>.createDataFrame()</code> method is expecting the data to be oriented by row. Meaning that the data should be in the form of a list of dictionaries, where each dictionary represents a row of data. However, we currently have our data is oriented by column, where the dictionarieshave keys as column names and values as lists of data. Therefore, we will first need to convert the dictionaries from column orientation to row orientation. The easiest way to do this is by parse'ing the data to a Pandas DataFrames, and then using that to create our PySpark DataFrames from there.</p> <p>A good description of how to create PySpark DataFrames from Python Dictionaries can be found in the PySpark documentation: PySpark Create DataFrame From Dictionary.</p> Create DataFrames<pre><code>df_sales_ps: psDataFrame = spark.createDataFrame(pd.DataFrame(sales_data))\ndf_product_ps: psDataFrame = spark.createDataFrame(pd.DataFrame(product_data))\ndf_customer_ps: psDataFrame = spark.createDataFrame(pd.DataFrame(customer_data))\n</code></pre> <p>Once the data is created, we can check that it has been loaded correctly by displaying the first few rows of each DataFrame. To do this, we will use the <code>.show()</code> method to display the first <code>5</code> rows of each DataFrame. The <code>.show()</code> method is used to display the data in a tabular format, similar to how it would be displayed in a SQL database.</p> Check Sales DataFrame<pre><code>print(f\"Sales DataFrame: {df_sales_ps.count()}\")\ndf_sales_ps.show(5)\nprint(df_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Sales DataFrame: 100\n</code></pre> <pre><code>+-------------------+-----------+----------+-----------+------------+--------+\n|               date|customer_id|product_id|   category|sales_amount|quantity|\n+-------------------+-----------+----------+-----------+------------+--------+\n|2023-01-01 00:00:00|         52|        45|       Food|      490.76|       7|\n|2023-01-02 00:00:00|         93|        41|Electronics|      453.94|       5|\n|2023-01-03 00:00:00|         15|        29|       Home|      994.51|       5|\n|2023-01-04 00:00:00|         72|        15|Electronics|      184.17|       7|\n|2023-01-05 00:00:00|         61|        45|       Food|       27.89|       9|\n+-------------------+-----------+----------+-----------+------------+--------+\nonly showing top 10 rows\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-01 00:00:00 52 45 Food 490.76 7 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 2 2023-01-03 00:00:00 15 29 Home 994.51 5 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Check Product DataFrame<pre><code>print(f\"Product DataFrame: {df_product_ps.count()}\")\ndf_product_ps.show(5)\nprint(df_product_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Product DataFrame: 50\n</code></pre> <pre><code>+----------+------------+------+--------+-----------+\n|product_id|product_name| price|category|supplier_id|\n+----------+------------+------+--------+-----------+\n|         1|   Product 1|257.57|    Food|          8|\n|         2|   Product 2|414.96|Clothing|          5|\n|         3|   Product 3|166.82|Clothing|          8|\n|         4|   Product 4|448.81|    Food|          4|\n|         5|   Product 5|200.71|    Food|          8|\n+----------+------------+------+--------+-----------+\nonly showing top 5 rows\n</code></pre> product_id product_name price category supplier_id 0 1 Product 1 257.57 Food 8 1 2 Product 2 414.96 Clothing 5 2 3 Product 3 166.82 Clothing 8 3 4 Product 4 448.81 Food 4 4 5 Product 5 200.71 Food 8 Check Customer DataFrame<pre><code>print(f\"Customer DataFrame: {df_customer_ps.count()}\")\ndf_customer_ps.show(5)\nprint(df_customer_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Customer DataFrame: 100\n</code></pre> <pre><code>+-----------+-------------+-----------+-----+-----------+\n|customer_id|customer_name|       city|state|    segment|\n+-----------+-------------+-----------+-----+-----------+\n|          1|   Customer 1|    Phoenix|   NY|  Corporate|\n|          2|   Customer 2|    Phoenix|   CA|Home Office|\n|          3|   Customer 3|    Phoenix|   NY|Home Office|\n|          4|   Customer 4|Los Angeles|   NY|   Consumer|\n|          5|   Customer 5|Los Angeles|   IL|Home Office|\n+-----------+-------------+-----------+-----+-----------+\nonly showing top 5 rows\n</code></pre> customer_id customer_name city state segment 0 1 Customer 1 Phoenix NY Corporate 1 2 Customer 2 Phoenix CA Home Office 2 3 Customer 3 Phoenix NY Home Office 3 4 Customer 4 Los Angeles NY Consumer 4 5 Customer 5 Los Angeles IL Home Office <p>To create the dataframes in Polars, we will use the data we generated earlier. We will parse the dictionaries into Polars DataFrames, which will allow us to perform various data manipulation tasks.</p> Create DataFrames<pre><code>df_sales_pl: pl.DataFrame = pl.DataFrame(sales_data)\ndf_product_pl: pl.DataFrame = pl.DataFrame(product_data)\ndf_customer_pl: pl.DataFrame = pl.DataFrame(customer_data)\n</code></pre> <p>Once the data is created, we can check that it has been loaded correctly by displaying the first few rows of each DataFrame. To do this, we will use the <code>.head()</code> method to display the first <code>5</code> rows of each DataFrame, and then parse to the <code>print()</code> function to display the DataFrame in a readable format.</p> Check Sales DataFrame<pre><code>print(f\"Sales DataFrame: {df_sales_pl.shape[0]}\")\nprint(df_sales_pl.head(5))\nprint(df_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Sales DataFrame: 100\n</code></pre> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date                \u2506 customer_id \u2506 product_id \u2506 category    \u2506 sales_amount \u2506 quantity \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---        \u2506 ---         \u2506 ---          \u2506 ---      \u2502\n\u2502 datetime[ns]        \u2506 i64         \u2506 i64        \u2506 str         \u2506 f64          \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 00:00:00 \u2506 52          \u2506 45         \u2506 Food        \u2506 490.76       \u2506 7        \u2502\n\u2502 2023-01-02 00:00:00 \u2506 93          \u2506 41         \u2506 Electronics \u2506 453.94       \u2506 5        \u2502\n\u2502 2023-01-03 00:00:00 \u2506 15          \u2506 29         \u2506 Home        \u2506 994.51       \u2506 5        \u2502\n\u2502 2023-01-04 00:00:00 \u2506 72          \u2506 15         \u2506 Electronics \u2506 184.17       \u2506 7        \u2502\n\u2502 2023-01-05 00:00:00 \u2506 61          \u2506 45         \u2506 Food        \u2506 27.89        \u2506 9        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-01 00:00:00 52 45 Food 490.76 7 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 2 2023-01-03 00:00:00 15 29 Home 994.51 5 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Check Product DataFrame<pre><code>print(f\"Product DataFrame: {df_product_pl.shape[0]}\")\nprint(df_product_pl.head(5))\nprint(df_product_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Product DataFrame: 50\n</code></pre> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 product_id \u2506 product_name \u2506 price  \u2506 category \u2506 supplier_id \u2502\n\u2502 ---        \u2506 ---          \u2506 ---    \u2506 ---      \u2506 ---         \u2502\n\u2502 i64        \u2506 str          \u2506 f64    \u2506 str      \u2506 i64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1          \u2506 Product 1    \u2506 257.57 \u2506 Food     \u2506 8           \u2502\n\u2502 2          \u2506 Product 2    \u2506 414.96 \u2506 Clothing \u2506 5           \u2502\n\u2502 3          \u2506 Product 3    \u2506 166.82 \u2506 Clothing \u2506 8           \u2502\n\u2502 4          \u2506 Product 4    \u2506 448.81 \u2506 Food     \u2506 4           \u2502\n\u2502 5          \u2506 Product 5    \u2506 200.71 \u2506 Food     \u2506 8           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> product_id product_name price category supplier_id 0 1 Product 1 257.57 Food 8 1 2 Product 2 414.96 Clothing 5 2 3 Product 3 166.82 Clothing 8 3 4 Product 4 448.81 Food 4 4 5 Product 5 200.71 Food 8 Check Customer DataFrame<pre><code>print(f\"Customer DataFrame: {df_customer_pl.shape[0]}\")\nprint(df_customer_pl.head(5))\nprint(df_customer_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Customer DataFrame: 100\n</code></pre> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 customer_name \u2506 city        \u2506 state \u2506 segment     \u2502\n\u2502 ---         \u2506 ---           \u2506 ---         \u2506 ---   \u2506 ---         \u2502\n\u2502 i64         \u2506 str           \u2506 str         \u2506 str   \u2506 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Customer 1    \u2506 Phoenix     \u2506 NY    \u2506 Corporate   \u2502\n\u2502 2           \u2506 Customer 2    \u2506 Phoenix     \u2506 CA    \u2506 Home Office \u2502\n\u2502 3           \u2506 Customer 3    \u2506 Phoenix     \u2506 NY    \u2506 Home Office \u2502\n\u2502 4           \u2506 Customer 4    \u2506 Los Angeles \u2506 NY    \u2506 Consumer    \u2502\n\u2502 5           \u2506 Customer 5    \u2506 Los Angeles \u2506 IL    \u2506 Home Office \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> customer_id customer_name city state segment 0 1 Customer 1 Phoenix NY Corporate 1 2 Customer 2 Phoenix CA Home Office 2 3 Customer 3 Phoenix NY Home Office 3 4 Customer 4 Los Angeles NY Consumer 4 5 Customer 5 Los Angeles IL Home Office"},{"location":"guides/querying-data/#1-filtering-and-selecting","title":"1. Filtering and Selecting","text":"<p>This first section will demonstrate how to filter and select data from the DataFrames. This is a common operation in data analysis, allowing us to focus on specific subsets of the data.</p> PandasSQLPySparkPolars <p>In Pandas, we can use boolean indexing to filter rows based on specific conditions. As you can see in this first example, this looks like using square brackets, within which we define a column and a condition. In the below example, we can use string values to filter categorical data.</p> <p>For more information about filtering in Pandas, see the Pandas documentation on filtering.</p> Filter sales data for specific category<pre><code>electronics_sales_pd: pd.DataFrame = df_sales_pd[df_sales_pd[\"category\"] == \"Electronics\"]\nprint(f\"Number of Electronics Sales: {len(electronics_sales_pd)}\")\nprint(electronics_sales_pd.head(5))\nprint(electronics_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Number of Electronics Sales: 28\n</code></pre> <pre><code>         date  customer_id  product_id     category  sales_amount  quantity\n1  2023-01-02           93          41  Electronics        453.94         5\n3  2023-01-04           72          15  Electronics        184.17         7\n8  2023-01-09           75           9  Electronics        746.73         2\n10 2023-01-11           88           1  Electronics        314.98         9\n11 2023-01-12           24          44  Electronics        547.11         8\n</code></pre> date customer_id product_id category sales_amount quantity 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 8 2023-01-09 00:00:00 75 9 Electronics 746.73 2 10 2023-01-11 00:00:00 88 1 Electronics 314.98 9 11 2023-01-12 00:00:00 24 44 Electronics 547.11 8 <p>In SQL, we can use the <code>WHERE</code> clause to filter rows based on specific conditions. The syntax should be very familiar to anyone who has worked with SQL before. We can use the <code>pd.read_sql()</code> function to execute SQL queries and retrieve the data from the database. The result is a Pandas DataFrame that contains only the rows that match the specified condition. In the below example, we filter for sales in the \"Electronics\" category.</p> <p>For more information about filtering in SQL, see the SQL WHERE clause documentation.</p> Filter sales for a specific category<pre><code>electronics_sales_txt: str = \"\"\"\n    SELECT *\n    FROM sales\n    WHERE category = 'Electronics'\n\"\"\"\nelectronics_sales_sql: pd.DataFrame = pd.read_sql(electronics_sales_txt, conn)\nprint(f\"Number of Electronics Sales: {len(electronics_sales_sql)}\")\nprint(pd.read_sql(electronics_sales_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(electronics_sales_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Number of Electronics Sales: 28\n</code></pre> <pre><code>                  date  customer_id  product_id     category  sales_amount  quantity\n0  2023-01-02 00:00:00           93          41  Electronics        453.94         5\n1  2023-01-04 00:00:00           72          15  Electronics        184.17         7\n2  2023-01-09 00:00:00           75           9  Electronics        746.73         2\n3  2023-01-11 00:00:00           88           1  Electronics        314.98         9\n4  2023-01-12 00:00:00           24          44  Electronics        547.11         8\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-02 00:00:00 93 41 Electronics 453.94 5 1 2023-01-04 00:00:00 72 15 Electronics 184.17 7 2 2023-01-09 00:00:00 75 9 Electronics 746.73 2 3 2023-01-11 00:00:00 88 1 Electronics 314.98 9 4 2023-01-12 00:00:00 24 44 Electronics 547.11 8 <p>In PySpark, we can use the <code>.filter()</code> (or the <code>.where()</code>) method to filter rows based on specific conditions. This process is effectively doing a boolean indexing operation to filter the DataFrame. The syntax is similar to SQL, where we can specify the condition as a string or using column expressions. In the below example, we filter for sales in the \"Electronics\" category.</p> <p>For more information about filtering in PySpark, see the PySpark documentation on filtering.</p> Filter sales for a specific category<pre><code>electronics_sales_ps: psDataFrame = df_sales_ps.filter(df_sales_ps[\"category\"] == \"Electronics\")\nprint(f\"Number of Electronics Sales: {electronics_sales_ps.count()}\")\nelectronics_sales_ps.show(5)\nprint(electronics_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Number of Electronics Sales: 28\n</code></pre> <pre><code>+-------------------+-----------+----------+-----------+------------+--------+\n|               date|customer_id|product_id|   category|sales_amount|quantity|\n+-------------------+-----------+----------+-----------+------------+--------+\n|2023-01-02 00:00:00|         93|        41|Electronics|      453.94|       5|\n|2023-01-04 00:00:00|         72|        15|Electronics|      184.17|       7|\n|2023-01-09 00:00:00|         75|         9|Electronics|      746.73|       2|\n|2023-01-11 00:00:00|         88|         1|Electronics|      314.98|       9|\n|2023-01-12 00:00:00|         24|        44|Electronics|      547.11|       8|\n+-------------------+-----------+----------+-----------+------------+--------+\nonly showing top 5 rows\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-02 00:00:00 93 41 Electronics 453.94 5 1 2023-01-04 00:00:00 72 15 Electronics 184.17 7 2 2023-01-09 00:00:00 75 9 Electronics 746.73 2 3 2023-01-11 00:00:00 88 1 Electronics 314.98 9 4 2023-01-12 00:00:00 24 44 Electronics 547.11 8 <p>In Polars, we can use the <code>.filter()</code> method to filter rows based on specific conditions. The syntax is similar to Pandas, where we can specify the condition using column expressions. In the below example, we filter for sales in the \"Electronics\" category.</p> <p>For more information about filtering in Polars, see the Polars documentation on filtering.</p> Filter sales for a specific category<pre><code>electronics_sales_pl: pl.DataFrame = df_sales_pl.filter(df_sales_pl[\"category\"] == \"Electronics\")\nprint(f\"Number of Electronics Sales: {len(electronics_sales_pl)}\")\nprint(electronics_sales_pl.head(5))\nprint(electronics_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Number of Electronics Sales: 28\n</code></pre> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date                \u2506 customer_id \u2506 product_id \u2506 category    \u2506 sales_amount \u2506 quantity \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---        \u2506 ---         \u2506 ---          \u2506 ---      \u2502\n\u2502 datetime[ns]        \u2506 i64         \u2506 i64        \u2506 str         \u2506 f64          \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-02 00:00:00 \u2506 93          \u2506 41         \u2506 Electronics \u2506 453.94       \u2506 5        \u2502\n\u2502 2023-01-04 00:00:00 \u2506 72          \u2506 15         \u2506 Electronics \u2506 184.17       \u2506 7        \u2502\n\u2502 2023-01-09 00:00:00 \u2506 75          \u2506 9          \u2506 Electronics \u2506 746.73       \u2506 2        \u2502\n\u2502 2023-01-11 00:00:00 \u2506 88          \u2506 1          \u2506 Electronics \u2506 314.98       \u2506 9        \u2502\n\u2502 2023-01-12 00:00:00 \u2506 24          \u2506 44         \u2506 Electronics \u2506 547.11       \u2506 8        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-02 00:00:00 93 41 Electronics 453.94 5 1 2023-01-04 00:00:00 72 15 Electronics 184.17 7 2 2023-01-09 00:00:00 75 9 Electronics 746.73 2 3 2023-01-11 00:00:00 88 1 Electronics 314.98 9 4 2023-01-12 00:00:00 24 44 Electronics 547.11 8 <p>We can also use numerical filtering, as you can see in the next example, where we filter for sales amounts greater than $500.</p> PandasSQLPySparkPolars <p>When it comes to numerical filtering in Pandas, the process is similar to the previous example, where we use boolean indexing to filter rows based on a given condition condition, but here we use a numerical value instead of a string value. In the below example, we filter for sales amounts greater than <code>500</code>.</p> Filter for high value transactions<pre><code>high_value_sales_pd: pd.DataFrame = df_sales_pd[df_sales_pd[\"sales_amount\"] &gt; 500]\nprint(f\"Number of high-value Sales: {len(high_value_sales_pd)}\")\nprint(high_value_sales_pd.head(5))\nprint(high_value_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Number of high-value Sales: 43\n</code></pre> <pre><code>         date  customer_id  product_id     category  sales_amount  quantity\n2  2023-01-03           15          29         Home        994.51         5\n8  2023-01-09           75           9  Electronics        746.73         2\n9  2023-01-10           75          24        Books        723.73         6\n11 2023-01-12           24          44  Electronics        547.11         8\n12 2023-01-13            3           8     Clothing        513.73         5\n</code></pre> date customer_id product_id category sales_amount quantity 2 2023-01-03 00:00:00 15 29 Home 994.51 5 8 2023-01-09 00:00:00 75 9 Electronics 746.73 2 9 2023-01-10 00:00:00 75 24 Books 723.73 6 11 2023-01-12 00:00:00 24 44 Electronics 547.11 8 12 2023-01-13 00:00:00 3 8 Clothing 513.73 5 <p>When it comes to numerical filtering in SQL, the process is similar to the previous example, where we use the <code>WHERE</code> clause to filter rows based on a given condition, but here we use a numerical value instead of a string value. In the below example, we filter for sales amounts greater than <code>500</code>.</p> Filter for high value transactions<pre><code>high_value_sales_txt: str = \"\"\"\n    SELECT *\n    FROM sales\n    WHERE sales_amount &gt; 500\n\"\"\"\nhigh_value_sales_sql: pd.DataFrame = pd.read_sql(high_value_sales_txt, conn)\nprint(f\"Number of high-value Sales: {len(high_value_sales_sql)}\")\nprint(pd.read_sql(high_value_sales_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(high_value_sales_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Number of high-value Sales: 43\n</code></pre> <pre><code>                  date  customer_id  product_id     category  sales_amount  quantity\n0  2023-01-03 00:00:00           15          29         Home        994.51         5\n1  2023-01-09 00:00:00           75           9  Electronics        746.73         2\n2  2023-01-10 00:00:00           75          24        Books        723.73         6\n3  2023-01-12 00:00:00           24          44  Electronics        547.11         8\n4  2023-01-13 00:00:00            3           8     Clothing        513.73         5\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-03 00:00:00 15 29 Home 994.51 5 1 2023-01-09 00:00:00 75 9 Electronics 746.73 2 2 2023-01-10 00:00:00 75 24 Books 723.73 6 3 2023-01-12 00:00:00 24 44 Electronics 547.11 8 4 2023-01-13 00:00:00 3 8 Clothing 513.73 5 <p>When it comes to numerical filtering in PySpark, the process is similar to the previous example, where we use the <code>.filter()</code> (or <code>.where()</code>) method to filter rows based on a given condition, but here we use a numerical value instead of a string value. In the below example, we filter for sales amounts greater than <code>500</code>.</p> <p>Also note here that we have parsed a string value to the <code>.filter()</code> method, instead of using the pure-Python syntax as shown above. This is because the <code>.filter()</code> method can accept a SQL-like string expression. This is a common practice in PySpark to parse a SQL-like string to a PySpark method.</p> Filter for high value transactions<pre><code>high_value_sales_ps: psDataFrame = df_sales_ps.filter(\"sales_amount &gt; 500\")\nprint(f\"Number of high-value Sales: {high_value_sales_ps.count()}\")\nhigh_value_sales_ps.show(5)\nprint(high_value_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Number of high-value Sales: 43\n</code></pre> <pre><code>+-------------------+-----------+----------+-----------+------------+--------+\n|               date|customer_id|product_id|   category|sales_amount|quantity|\n+-------------------+-----------+----------+-----------+------------+--------+\n|2023-01-03 00:00:00|         15|        29|       Home|      994.51|       5|\n|2023-01-09 00:00:00|         75|         9|Electronics|      746.73|       2|\n|2023-01-10 00:00:00|         75|        24|      Books|      723.73|       6|\n|2023-01-12 00:00:00|         24|        44|Electronics|      547.11|       8|\n|2023-01-13 00:00:00|          3|         8|   Clothing|      513.73|       5|\n+-------------------+-----------+----------+-----------+------------+--------+\nonly showing top 5 rows\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-03 00:00:00 15 29 Home 994.51 5 1 2023-01-09 00:00:00 75 9 Electronics 746.73 2 2 2023-01-10 00:00:00 75 24 Books 723.73 6 3 2023-01-12 00:00:00 24 44 Electronics 547.11 8 4 2023-01-13 00:00:00 3 8 Clothing 513.73 5 <p>When it comes to numerical filtering in Polars, the process is similar to the previous example, where we use the <code>.filter()</code> method to filter rows based on a given condition, but here we use a numerical value instead of a string value. In the below example, we filter for sales amounts greater than <code>500</code>.</p> <p>Also note here that we have used the <code>pl.col()</code> function to specify the column we want to filter on. This is different from the previous examples, where we used the column name directly. The use of <code>pl.col()</code> is a common practice in Polars to specify the column name in a more readable way.</p> Filter for high value transactions<pre><code>high_value_sales_pl: pl.DataFrame = df_sales_pl.filter(pl.col(\"sales_amount\") &gt; 500)\nprint(f\"Number of high-value Sales: {len(high_value_sales_pl)}\")\nprint(high_value_sales_pl.head(5))\nprint(high_value_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Number of high-value Sales: 43\n</code></pre> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date                \u2506 customer_id \u2506 product_id \u2506 category    \u2506 sales_amount \u2506 quantity \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---        \u2506 ---         \u2506 ---          \u2506 ---      \u2502\n\u2502 datetime[ns]        \u2506 i64         \u2506 i64        \u2506 str         \u2506 f64          \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-03 00:00:00 \u2506 15          \u2506 29         \u2506 Home        \u2506 994.51       \u2506 5        \u2502\n\u2502 2023-01-09 00:00:00 \u2506 75          \u2506 9          \u2506 Electronics \u2506 746.73       \u2506 2        \u2502\n\u2502 2023-01-10 00:00:00 \u2506 75          \u2506 24         \u2506 Books       \u2506 723.73       \u2506 6        \u2502\n\u2502 2023-01-12 00:00:00 \u2506 24          \u2506 44         \u2506 Electronics \u2506 547.11       \u2506 8        \u2502\n\u2502 2023-01-13 00:00:00 \u2506 3           \u2506 8          \u2506 Clothing    \u2506 513.73       \u2506 5        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date customer_id product_id category sales_amount quantity 0 2023-01-03 00:00:00 15 29 Home 994.51 5 1 2023-01-09 00:00:00 75 9 Electronics 746.73 2 2 2023-01-10 00:00:00 75 24 Books 723.73 6 3 2023-01-12 00:00:00 24 44 Electronics 547.11 8 4 2023-01-13 00:00:00 3 8 Clothing 513.73 5 <p>In addition to subsetting a table by rows (aka filtering), we can also subset a table by columns (aka selecting). This allows us to create a new DataFrame with only the relevant columns we want to work with. This is useful when we want to focus on specific attributes of the data, such as dates, categories, or sales amounts.</p> PandasSQLPySparkPolars <p>To select specific columns in Pandas, we can use the double square brackets syntax to specify the columns we want to keep in the DataFrame. This allows us to create a new DataFrame with only the relevant columns.</p> <p>For more information about selecting specific columns, see the Pandas documentation on selecting columns.</p> Select specific columns<pre><code>sales_summary_pd: pd.DataFrame = df_sales_pd[[\"date\", \"category\", \"sales_amount\"]]\nprint(f\"Sales Summary DataFrame: {len(sales_summary_pd)}\")\nprint(sales_summary_pd.head(5))\nprint(sales_summary_pd.head(5).to_markdown())\n</code></pre> <pre><code>Sales Summary DataFrame: 100\n</code></pre> <pre><code>        date     category  sales_amount\n0 2023-01-01         Food        490.76\n1 2023-01-02  Electronics        453.94\n2 2023-01-03         Home        994.51\n3 2023-01-04  Electronics        184.17\n4 2023-01-05         Food         27.89\n</code></pre> date category sales_amount 0 2023-01-01 00:00:00 Food 490.76 1 2023-01-02 00:00:00 Electronics 453.94 2 2023-01-03 00:00:00 Home 994.51 3 2023-01-04 00:00:00 Electronics 184.17 4 2023-01-05 00:00:00 Food 27.89 <p>To select specific columns in SQL, we can use the <code>SELECT</code> statement to specify the columns we want to retrieve from the table. This allows us to create a new DataFrame with only the relevant columns. We can use the <code>pd.read_sql()</code> function to execute SQL queries and retrieve the data from the database.</p> <p>For more information about selecting specific columns in SQL, see the SQL SELECT statement documentation.</p> Select specific columns<pre><code>sales_summary_txt: str = \"\"\"\n    SELECT date, category, sales_amount\n    FROM sales\n\"\"\"\nsales_summary_sql: pd.DataFrame = pd.read_sql(sales_summary_txt, conn)\nprint(f\"Selected columns in Sales: {len(sales_summary_sql)}\")\nprint(pd.read_sql(sales_summary_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(sales_summary_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Selected columns in Sales: 100\n</code></pre> <pre><code>                  date     category  sales_amount\n0  2023-01-01 00:00:00         Food        490.76\n1  2023-01-02 00:00:00  Electronics        453.94\n2  2023-01-03 00:00:00         Home        994.51\n3  2023-01-04 00:00:00  Electronics        184.17\n4  2023-01-05 00:00:00         Food         27.89\n</code></pre> date category sales_amount 0 2023-01-01 00:00:00 Food 490.76 1 2023-01-02 00:00:00 Electronics 453.94 2 2023-01-03 00:00:00 Home 994.51 3 2023-01-04 00:00:00 Electronics 184.17 4 2023-01-05 00:00:00 Food 27.89 <p>To select specific columns in PySpark, we can use the <code>.select()</code> method to specify the columns we want to keep in the DataFrame. This allows us to create a new DataFrame with only the relevant columns. The syntax is similar to SQL, where we can specify the column names as strings.</p> Select specific columns<pre><code>sales_summary_ps: psDataFrame = df_sales_ps.select(\"date\", \"category\", \"sales_amount\")\nprint(f\"Sales Summary DataFrame: {sales_summary_ps.count()}\")\nsales_summary_ps.show(5)\nprint(sales_summary_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Sales Summary DataFrame: 100\n</code></pre> <pre><code>+-------------------+-----------+------------+\n|               date|   category|sales_amount|\n+-------------------+-----------+------------+\n|2023-01-01 00:00:00|       Food|      490.76|\n|2023-01-02 00:00:00|Electronics|      453.94|\n|2023-01-03 00:00:00|       Home|      994.51|\n|2023-01-04 00:00:00|Electronics|      184.17|\n|2023-01-05 00:00:00|       Food|       27.89|\n+-------------------+-----------+------------+\nonly showing top 5 rows\n</code></pre> date category sales_amount 0 2023-01-01 00:00:00 Food 490.76 1 2023-01-02 00:00:00 Electronics 453.94 2 2023-01-03 00:00:00 Home 994.51 3 2023-01-04 00:00:00 Electronics 184.17 4 2023-01-05 00:00:00 Food 27.89 <p>To select specific columns in Polars, we can use the <code>.select()</code> method to specify the columns we want to keep in the DataFrame. This allows us to create a new DataFrame with only the relevant columns.</p> Select specific columns<pre><code>sales_summary_pl: pl.DataFrame = df_sales_pl.select([\"date\", \"category\", \"sales_amount\"])\nprint(f\"Sales Summary DataFrame: {len(sales_summary_pl)}\")\nprint(sales_summary_pl.head(5))\nprint(sales_summary_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Sales Summary DataFrame: 100\n</code></pre> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date                \u2506 category    \u2506 sales_amount \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---          \u2502\n\u2502 datetime[ns]        \u2506 str         \u2506 f64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 00:00:00 \u2506 Food        \u2506 490.76       \u2502\n\u2502 2023-01-02 00:00:00 \u2506 Electronics \u2506 453.94       \u2502\n\u2502 2023-01-03 00:00:00 \u2506 Home        \u2506 994.51       \u2502\n\u2502 2023-01-04 00:00:00 \u2506 Electronics \u2506 184.17       \u2502\n\u2502 2023-01-05 00:00:00 \u2506 Food        \u2506 27.89        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date category sales_amount 0 2023-01-01 00:00:00 Food 490.76 1 2023-01-02 00:00:00 Electronics 453.94 2 2023-01-03 00:00:00 Home 994.51 3 2023-01-04 00:00:00 Electronics 184.17 4 2023-01-05 00:00:00 Food 27.89"},{"location":"guides/querying-data/#2-grouping-and-aggregation","title":"2. Grouping and Aggregation","text":"<p>The second section will cover grouping and aggregation techniques. These operations are essential for summarizing data and extracting insights from large datasets.</p> PandasSQLPySparkPolars <p>In Pandas, we can use the <code>.agg()</code> method to perform aggregation operations on DataFrames. This method allows us to apply multiple aggregation functions to different columns in a single operation.</p> Basic aggregation<pre><code>sales_stats_pd: pd.DataFrame = df_sales_pd.agg(\n    {\n        \"sales_amount\": [\"sum\", \"mean\", \"min\", \"max\", \"count\"],\n        \"quantity\": [\"sum\", \"mean\", \"min\", \"max\"],\n    }\n)\nprint(f\"Sales Statistics: {len(sales_stats_pd)}\")\nprint(sales_stats_pd)\nprint(sales_stats_pd.to_markdown())\n</code></pre> <pre><code>Sales Statistics: 5\n</code></pre> <pre><code>       sales_amount  quantity\nsum      48227.0500    464.00\nmean       482.2705      4.64\nmin         15.1300      1.00\nmax        994.6100      9.00\ncount      100.0000       NaN\n</code></pre> sales_amount quantity sum 48227.1 464 mean 482.271 4.64 min 15.13 1 max 994.61 9 count 100 nan <p>In SQL, we can use the aggregate functions like <code>SUM()</code>, <code>AVG()</code>, <code>MIN()</code>, <code>MAX()</code>, and <code>COUNT()</code> to perform aggregation operations on tables.</p> <p>Note here that we are not using the <code>GROUP BY</code> clause, which is typically used to group rows that have the same values in specified columns into summary rows. Instead, we are performing a basic aggregation on the entire table.</p> Basic aggregation<pre><code>sales_stats_txt: str = \"\"\"\n    SELECT\n        SUM(sales_amount) AS sales_sum,\n        AVG(sales_amount) AS sales_mean,\n        MIN(sales_amount) AS sales_min,\n        MAX(sales_amount) AS sales_max,\n        COUNT(*) AS sales_count,\n        SUM(quantity) AS quantity_sum,\n        AVG(quantity) AS quantity_mean,\n        MIN(quantity) AS quantity_min,\n        MAX(quantity) AS quantity_max\n    FROM sales\n\"\"\"\nprint(f\"Sales Statistics: {len(pd.read_sql(sales_stats_txt, conn))}\")\nprint(pd.read_sql(sales_stats_txt, conn))\nprint(pd.read_sql(sales_stats_txt, conn).to_markdown())\n</code></pre> <pre><code>Sales Statistics: 1\n</code></pre> <pre><code>   sales_sum  sales_mean  sales_min  sales_max  sales_count  quantity_sum  quantity_mean  quantity_min  quantity_max\n0   48227.05    482.2705      15.13     994.61          100           464           4.64             1             9\n</code></pre> sales_sum sales_mean sales_min sales_max sales_count quantity_sum quantity_mean quantity_min quantity_max 0 48227.1 482.271 15.13 994.61 100 464 4.64 1 9 <p>In PySpark, we can use the <code>.agg()</code> method to perform aggregation operations on DataFrames. This method allows us to apply multiple aggregation functions to different columns in a single operation.</p> Basic aggregation<pre><code>sales_stats_ps: psDataFrame = df_sales_ps.agg(\n    F.sum(\"sales_amount\").alias(\"sales_sum\"),\n    F.avg(\"sales_amount\").alias(\"sales_mean\"),\n    F.expr(\"MIN(sales_amount) AS sales_min\"),\n    F.expr(\"MAX(sales_amount) AS sales_max\"),\n    F.count(\"*\").alias(\"sales_count\"),\n    F.expr(\"SUM(quantity) AS quantity_sum\"),\n    F.expr(\"AVG(quantity) AS quantity_mean\"),\n    F.min(\"quantity\").alias(\"quantity_min\"),\n    F.max(\"quantity\").alias(\"quantity_max\"),\n)\nprint(f\"Sales Statistics: {sales_stats_ps.count()}\")\nsales_stats_ps.show(5)\nprint(sales_stats_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Sales Statistics: 1\n</code></pre> <pre><code>+---------+----------+---------+---------+-----------+------------+-------------+------------+------------+\n|sales_sum|sales_mean|sales_min|sales_max|sales_count|quantity_sum|quantity_mean|quantity_min|quantity_max|\n+---------+----------+---------+---------+-----------+------------+-------------+------------+------------+\n| 48227.05|  482.2705|    15.13|   994.61|        100|         464|         4.64|           1|           9|\n+---------+----------+---------+---------+-----------+------------+-------------+------------+------------+\n</code></pre> sales_sum sales_mean sales_min sales_max sales_count quantity_sum quantity_mean quantity_min quantity_max 0 48227.1 482.271 15.13 994.61 100 464 4.64 1 9 <p>In Polars, we can use the <code>.select()</code> method to perform aggregation operations on DataFrames. This method allows us to apply multiple aggregation functions to different columns in a single operation.</p> Basic aggregation<pre><code>sales_stats_pl: pl.DataFrame = df_sales_pl.select(\n    pl.col(\"sales_amount\").sum().alias(\"sales_sum\"),\n    pl.col(\"sales_amount\").mean().alias(\"sales_mean\"),\n    pl.col(\"sales_amount\").min().alias(\"sales_min\"),\n    pl.col(\"sales_amount\").max().alias(\"sales_max\"),\n    pl.col(\"quantity\").sum().alias(\"quantity_sum\"),\n    pl.col(\"quantity\").mean().alias(\"quantity_mean\"),\n    pl.col(\"quantity\").min().alias(\"quantity_min\"),\n    pl.col(\"quantity\").max().alias(\"quantity_max\"),\n)\nprint(f\"Sales Statistics: {len(sales_stats_pl)}\")\nprint(sales_stats_pl)\nprint(sales_stats_pl.to_pandas().to_markdown())\n</code></pre> <pre><code>Sales Statistics: 1\n</code></pre> <pre><code>shape: (1, 8)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sales_sum \u2506 sales_mean \u2506 sales_min \u2506 sales_max \u2506 quantity_sum \u2506 quantity_mean \u2506 quantity_min \u2506 quantity_max \u2502\n\u2502 ---       \u2506 ---        \u2506 ---       \u2506 ---       \u2506 ---          \u2506 ---           \u2506 ---          \u2506 ---          \u2502\n\u2502 f64       \u2506 f64        \u2506 f64       \u2506 f64       \u2506 i64          \u2506 f64           \u2506 i64          \u2506 i64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 48227.05  \u2506 482.2705   \u2506 15.13     \u2506 994.61    \u2506 464          \u2506 4.64          \u2506 1            \u2506 9            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> sales_sum sales_mean sales_min sales_max quantity_sum quantity_mean quantity_min quantity_max 0 48227 482.27 15.13 994.61 464 4.64 1 9 <p>It is also possible to group the data by a specific column and then apply aggregation functions to summarize the data by group.</p> PandasSQLPySparkPolars <p>This is done using the <code>.groupby()</code> method to group data by one or more columns and then apply aggregation functions to summarize the data, followed by the <code>.agg()</code> method.</p> Group by category and aggregate<pre><code>category_sales_pd: pd.DataFrame = df_sales_pd.groupby(\"category\").agg(\n    {\n        \"sales_amount\": [\"sum\", \"mean\", \"count\"],\n        \"quantity\": \"sum\",\n    }\n)\nprint(f\"Category Sales Summary: {len(category_sales_pd)}\")\nprint(category_sales_pd)\nprint(category_sales_pd.to_markdown())\n</code></pre> <pre><code>Category Sales Summary: 5\n</code></pre> <pre><code>            sales_amount                   quantity\n                     sum        mean count      sum\ncategory\nBooks           10154.83  441.514348    23      100\nClothing         7325.31  457.831875    16       62\nElectronics     11407.45  407.408929    28      147\nFood            12995.57  541.482083    24      115\nHome             6343.89  704.876667     9       40\n</code></pre> category ('sales_amount', 'sum') ('sales_amount', 'mean') ('sales_amount', 'count') ('quantity', 'sum') Books 10154.8 441.514 23 100 Clothing 7325.31 457.832 16 62 Electronics 11407.5 407.409 28 147 Food 12995.6 541.482 24 115 Home 6343.89 704.877 9 40 <p>In SQL, we can use the <code>GROUP BY</code> clause to group rows that have the same values in specified columns into summary rows. We can then apply aggregate functions like <code>SUM()</code>, <code>AVG()</code>, and <code>COUNT()</code> in the <code>SELECT</code> clause to summarize the data by group.</p> Group by category and aggregate<pre><code>category_sales_txt: str = \"\"\"\n    SELECT\n        category,\n        SUM(sales_amount) AS total_sales,\n        AVG(sales_amount) AS average_sales,\n        COUNT(*) AS transaction_count,\n        SUM(quantity) AS total_quantity\n    FROM sales\n    GROUP BY category\n\"\"\"\nprint(f\"Category Sales Summary: {len(pd.read_sql(category_sales_txt, conn))}\")\nprint(pd.read_sql(category_sales_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(category_sales_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Category Sales Summary: 5\n</code></pre> <pre><code>      category  total_sales  average_sales  transaction_count  total_quantity\n0        Books     10154.83     441.514348                 23             100\n1     Clothing      7325.31     457.831875                 16              62\n2  Electronics     11407.45     407.408929                 28             147\n3         Food     12995.57     541.482083                 24             115\n4         Home      6343.89     704.876667                  9              40\n</code></pre> category total_sales average_sales transaction_count total_quantity 0 Books 10154.8 441.514 23 100 1 Clothing 7325.31 457.832 16 62 2 Electronics 11407.5 407.409 28 147 3 Food 12995.6 541.482 24 115 4 Home 6343.89 704.877 9 40 <p>In PySpark, we can use the <code>.groupBy()</code> method to group data by one or more columns and then apply aggregation functions using the <code>.agg()</code> method.</p> Group by category and aggregate<pre><code>category_sales_ps: psDataFrame = df_sales_ps.groupBy(\"category\").agg(\n    F.sum(\"sales_amount\").alias(\"total_sales\"),\n    F.avg(\"sales_amount\").alias(\"average_sales\"),\n    F.count(\"*\").alias(\"transaction_count\"),\n    F.sum(\"quantity\").alias(\"total_quantity\"),\n)\nprint(f\"Category Sales Summary: {category_sales_ps.count()}\")\ncategory_sales_ps.show(5)\nprint(category_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Category Sales Summary: 5\n</code></pre> <pre><code>+-----------+------------------+------------------+-----------------+--------------+\n|   category|       total_sales|     average_sales|transaction_count|total_quantity|\n+-----------+------------------+------------------+-----------------+--------------+\n|       Home| 6343.889999999999| 704.8766666666666|                9|            40|\n|       Food|          12995.57| 541.4820833333333|               24|           115|\n|Electronics|11407.449999999999|407.40892857142853|               28|           147|\n|   Clothing|7325.3099999999995|457.83187499999997|               16|            62|\n|      Books|          10154.83|  441.514347826087|               23|           100|\n+-----------+------------------+------------------+-----------------+--------------+\n</code></pre> category total_sales average_sales transaction_count total_quantity 0 Home 6343.89 704.877 9 40 1 Food 12995.6 541.482 24 115 2 Electronics 11407.4 407.409 28 147 3 Clothing 7325.31 457.832 16 62 4 Books 10154.8 441.514 23 100 <p>In Polars, we can use the <code>.group_by()</code> method to group data by one or more columns and then apply aggregation functions using the <code>.agg()</code> method.</p> Group by category and aggregate<pre><code>category_sales_pl: pl.DataFrame = df_sales_pl.group_by(\"category\").agg(\n    pl.col(\"sales_amount\").sum().alias(\"total_sales\"),\n    pl.col(\"sales_amount\").mean().alias(\"average_sales\"),\n    pl.col(\"sales_amount\").count().alias(\"transaction_count\"),\n    pl.col(\"quantity\").sum().alias(\"total_quantity\"),\n)\nprint(f\"Category Sales Summary: {len(category_sales_pl)}\")\nprint(category_sales_pl.head(5))\nprint(category_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Category Sales Summary: 5\n</code></pre> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 category    \u2506 total_sales \u2506 average_sales \u2506 transaction_count \u2506 total_quantity \u2502\n\u2502 ---         \u2506 ---         \u2506 ---           \u2506 ---               \u2506 ---            \u2502\n\u2502 str         \u2506 f64         \u2506 f64           \u2506 u32               \u2506 i64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Food        \u2506 12995.57    \u2506 541.482083    \u2506 24                \u2506 115            \u2502\n\u2502 Electronics \u2506 11407.45    \u2506 407.408929    \u2506 28                \u2506 147            \u2502\n\u2502 Books       \u2506 10154.83    \u2506 441.514348    \u2506 23                \u2506 100            \u2502\n\u2502 Home        \u2506 6343.89     \u2506 704.876667    \u2506 9                 \u2506 40             \u2502\n\u2502 Clothing    \u2506 7325.31     \u2506 457.831875    \u2506 16                \u2506 62             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> category total_sales average_sales transaction_count total_quantity 0 Food 12995.6 541.482 24 115 1 Electronics 11407.5 407.409 28 147 2 Books 10154.8 441.514 23 100 3 Home 6343.89 704.877 9 40 4 Clothing 7325.31 457.832 16 62 <p>We can rename the columns for clarity by simply assigning new names.</p> PandasSQLPySparkPolars <p>In Pandas, we use the <code>.columns</code> attribute of the DataFrame. This makes it easier to understand the results of the aggregation.</p> <p>It's also possible to rename columns using the <code>.rename()</code> method, which allows for more flexibility in renaming specific columns from within 'dot-method' chains.</p> Rename columns for clarity<pre><code>category_sales_renamed_pd: pd.DataFrame = category_sales_pd.copy()\ncategory_sales_renamed_pd.columns = [\n    \"Total Sales\",\n    \"Average Sales\",\n    \"Transaction Count\",\n    \"Total Quantity\",\n]\nprint(f\"Renamed Category Sales Summary: {len(category_sales_renamed_pd)}\")\nprint(category_sales_renamed_pd.head(5))\nprint(category_sales_renamed_pd.head(5).to_markdown())\n</code></pre> <pre><code>Renamed Category Sales Summary: 5\n</code></pre> <pre><code>             Total Sales  Average Sales  Transaction Count  Total Quantity\ncategory\nBooks           10154.83     441.514348                 23             100\nClothing         7325.31     457.831875                 16              62\nElectronics     11407.45     407.408929                 28             147\nFood            12995.57     541.482083                 24             115\nHome             6343.89     704.876667                  9              40\n</code></pre> category Total Sales Average Sales Transaction Count Total Quantity Books 10154.8 441.514 23 100 Clothing 7325.31 457.832 16 62 Electronics 11407.5 407.409 28 147 Food 12995.6 541.482 24 115 Home 6343.89 704.877 9 40 <p>In SQL, we can use the <code>AS</code> keyword to rename columns in the <code>SELECT</code> clause. This allows us to provide more descriptive names for the aggregated columns.</p> <p>In this example, we provide the same aggregation as before, but from within a subquery. Then, in the parent query, we rename the columns for clarity.</p> Rename columns for clarity<pre><code>category_sales_renamed_txt: str = \"\"\"\n    SELECT\n        category,\n        total_sales AS `Total Sales`,\n        average_sales AS `Average Sales`,\n        transaction_count AS `Transaction Count`,\n        total_quantity AS `Total Quantity`\n    FROM (\n        SELECT\n            category,\n            SUM(sales_amount) AS total_sales,\n            AVG(sales_amount) AS average_sales,\n            COUNT(*) AS transaction_count,\n            SUM(quantity) AS total_quantity\n        FROM sales\n        GROUP BY category\n    ) AS sales_summary\n\"\"\"\nprint(f\"Renamed Category Sales Summary: {len(pd.read_sql(category_sales_renamed_txt, conn))}\")\nprint(pd.read_sql(category_sales_renamed_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(category_sales_renamed_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Renamed Category Sales Summary: 5\n</code></pre> <pre><code>             Total Sales  Average Sales  Transaction Count  Total Quantity\ncategory\nBooks           10154.83     441.514348                 23             100\nClothing         7325.31     457.831875                 16              62\nElectronics     11407.45     407.408929                 28             147\nFood            12995.57     541.482083                 24             115\nHome             6343.89     704.876667                  9              40\n</code></pre> category Total Sales Average Sales Transaction Count Total Quantity Books 10154.8 441.514 23 100 Clothing 7325.31 457.832 16 62 Electronics 11407.5 407.409 28 147 Food 12995.6 541.482 24 115 Home 6343.89 704.877 9 40 <p>In PySpark, we can use the <code>.withColumnsRenamed()</code> method to rename columns in a DataFrame. This allows us to provide more descriptive names for the aggregated columns.</p> Rename columns for clarity<pre><code>category_sales_renamed_ps: psDataFrame = category_sales_ps.withColumnsRenamed(\n    {\n        \"total_sales\": \"Total Sales\",\n        \"average_sales\": \"Average Sales\",\n        \"transaction_count\": \"Transaction Count\",\n        \"total_quantity\": \"Total Quantity\",\n    }\n)\nprint(f\"Renamed Category Sales Summary: {category_sales_renamed_ps.count()}\")\ncategory_sales_renamed_ps.show(5)\nprint(category_sales_renamed_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Renamed Category Sales Summary: 5\n</code></pre> <pre><code>+-----------+------------------+------------------+-----------------+--------------+\n|   category|       Total Sales|     Average Sales|Transaction Count|Total Quantity|\n+-----------+------------------+------------------+-----------------+--------------+\n|       Home| 6343.889999999999| 704.8766666666666|                9|            40|\n|       Food|          12995.57| 541.4820833333333|               24|           115|\n|Electronics|11407.449999999999|407.40892857142853|               28|           147|\n|   Clothing|7325.3099999999995|457.83187499999997|               16|            62|\n|      Books|          10154.83|  441.514347826087|               23|           100|\n+-----------+------------------+------------------+-----------------+--------------+\n</code></pre> category Total Sales Average Sales Transaction Count Total Quantity 0 Home 6343.89 704.877 9 40 1 Food 12995.6 541.482 24 115 2 Electronics 11407.4 407.409 28 147 3 Clothing 7325.31 457.832 16 62 4 Books 10154.8 441.514 23 100 <p>In Polars, we can use the <code>.rename()</code> method to rename columns in a DataFrame. This allows us to provide more descriptive names for the aggregated columns.</p> Rename columns for clarity<pre><code>category_sales_renamed_pl: pl.DataFrame = category_sales_pl.rename(\n    {\n        \"total_sales\": \"Total Sales\",\n        \"average_sales\": \"Average Sales\",\n        \"transaction_count\": \"Transaction Count\",\n        \"total_quantity\": \"Total Quantity\",\n    }\n)\nprint(f\"Renamed Category Sales Summary: {len(category_sales_renamed_pl)}\")\nprint(category_sales_renamed_pl.head(5))\nprint(category_sales_renamed_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Renamed Category Sales Summary: 5\n</code></pre> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 category    \u2506 Total Sales \u2506 Average Sales \u2506 Transaction Count \u2506 Total Quantity \u2502\n\u2502 ---         \u2506 ---         \u2506 ---           \u2506 ---               \u2506 ---            \u2502\n\u2502 str         \u2506 f64         \u2506 f64           \u2506 u32               \u2506 i64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Food        \u2506 12995.57    \u2506 541.482083    \u2506 24                \u2506 115            \u2502\n\u2502 Electronics \u2506 11407.45    \u2506 407.408929    \u2506 28                \u2506 147            \u2502\n\u2502 Books       \u2506 10154.83    \u2506 441.514348    \u2506 23                \u2506 100            \u2502\n\u2502 Home        \u2506 6343.89     \u2506 704.876667    \u2506 9                 \u2506 40             \u2502\n\u2502 Clothing    \u2506 7325.31     \u2506 457.831875    \u2506 16                \u2506 62             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> category Total Sales Average Sales Transaction Count Total Quantity 0 Food 12995.6 541.482 24 115 1 Electronics 11407.5 407.409 28 147 2 Books 10154.8 441.514 23 100 3 Home 6343.89 704.877 9 40 4 Clothing 7325.31 457.832 16 62 <p>Having aggregated the data, we can now visualize the results using Plotly. This allows us to create interactive visualizations that can help us better understand the data. The simplest way to do this is to use the Plotly Express module, which provides a high-level interface for creating visualizations. Here, we have utilised the <code>px.bar()</code> function to create a bar chart of the total sales by category.</p> PandasSQLPySparkPolars <p>The Plotly <code>px.bar()</code> function is able to receive a Pandas DataFrame directly, making it easy to create visualizations from the aggregated data. However, what we first need to do is to convert the index labels in to a column, so that we can use it as the x-axis in the bar chart. We do this with the <code>.reset_index()</code> method.</p> Plot the results<pre><code>fig: go.Figure = px.bar(\n    data_frame=category_sales_renamed_pd.reset_index(),\n    x=\"category\",\n    y=\"Total Sales\",\n    title=\"Total Sales by Category\",\n    text=\"Transaction Count\",\n    labels={\"Total Sales\": \"Total Sales ($)\", \"category\": \"Product Category\"},\n)\nfig.write_html(\"images/pt2_total_sales_by_category_pd.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p> <p>The Plotly <code>px.bar()</code> function can also receive a Pandas DataFrame, so we can use the results of the SQL query directly. Since the method we are using already returns the group labels in an individual column, we can use that directly in Plotly as the labels for the x-axis.</p> Plot the results<pre><code>fig: go.Figure = px.bar(\n    data_frame=pd.read_sql(category_sales_renamed_txt, conn),\n    x=\"category\",\n    y=\"Total Sales\",\n    title=\"Total Sales by Category\",\n    text=\"Transaction Count\",\n    labels={\"Total Sales\": \"Total Sales ($)\", \"category\": \"Product Category\"},\n)\nfig.write_html(\"images/pt2_total_sales_by_category_sql.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p> <p>Plotly is unfortunately not able to directly receive a PySpark DataFrame, so we need to convert it to a Pandas DataFrame first. This is done using the <code>.toPandas()</code> method, which converts the PySpark DataFrame to a Pandas DataFrame.</p> Plot the results<pre><code>fig: go.Figure = px.bar(\n    data_frame=category_sales_renamed_ps.toPandas(),\n    x=\"category\",\n    y=\"Total Sales\",\n    title=\"Total Sales by Category\",\n    text=\"Transaction Count\",\n    labels={\"Total Sales\": \"Total Sales ($)\", \"category\": \"Product Category\"},\n)\nfig.write_html(\"images/pt2_total_sales_by_category_ps.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p> <p>Plotly is also able to receive a Polars DataFrame, so we can use the results of the aggregation directly.</p> Plot the results<pre><code>fig: go.Figure = px.bar(\n    data_frame=category_sales_renamed_pl,\n    x=\"category\",\n    y=\"Total Sales\",\n    title=\"Total Sales by Category\",\n    text=\"Transaction Count\",\n    labels={\"Total Sales\": \"Total Sales ($)\", \"category\": \"Product Category\"},\n)\nfig.write_html(\"images/pt2_total_sales_by_category_pl.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p>"},{"location":"guides/querying-data/#3-joining","title":"3. Joining","text":"<p>The third section will demonstrate how to join DataFrames to combine data from different sources. This is a common operation in data analysis, allowing us to enrich our data with additional information.</p> <p>Here, we will join the <code>sales</code> DataFrame with the <code>product</code> DataFrame to get additional information about the products sold.</p> PandasSQLPySparkPolars <p>In Pandas, we can use the <code>pd.merge()</code> method to combine rows from two or more tables based on a related column between them. In this case, we will join the <code>sales</code> table with the <code>product</code> table on the <code>product_id</code> column.</p> Join sales with product data<pre><code>sales_with_product_pd: pd.DataFrame = pd.merge(\n    left=df_sales_pd,\n    right=df_product_pd[[\"product_id\", \"product_name\", \"price\"]],\n    on=\"product_id\",\n    how=\"left\",\n)\nprint(f\"Sales with Product Information: {len(sales_with_product_pd)}\")\nprint(sales_with_product_pd.head(5))\nprint(sales_with_product_pd.head(5).to_markdown())\n</code></pre> <pre><code>Sales with Product Information: 100\n</code></pre> <pre><code>        date  customer_id  product_id     category  sales_amount  quantity  product_name   price\n0 2023-01-01           52          45         Food        490.76         7    Product 45  493.14\n1 2023-01-02           93          41  Electronics        453.94         5    Product 41  193.39\n2 2023-01-03           15          29         Home        994.51         5    Product 29   80.07\n3 2023-01-04           72          15  Electronics        184.17         7    Product 15  153.67\n4 2023-01-05           61          45         Food         27.89         9    Product 45  493.14\n</code></pre> date customer_id product_id category sales_amount quantity product_name price 0 2023-01-01 00:00:00 52 45 Food 490.76 7 Product 45 493.14 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 Product 41 193.39 2 2023-01-03 00:00:00 15 29 Home 994.51 5 Product 29 80.07 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 Product 15 153.67 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Product 45 493.14 <p>In SQL, we can use the <code>JOIN</code> clause to combine rows from two or more tables based on a related column between them. In this case, we will join the <code>sales</code> table with the <code>product</code> table on the <code>product_id</code> column.</p> Join sales with product data<pre><code>sales_with_product_txt: str = \"\"\"\n    SELECT s.*, p.product_name, p.price\n    FROM sales s\n    LEFT JOIN product p ON s.product_id = p.product_id\n\"\"\"\nprint(f\"Sales with Product Information: {len(pd.read_sql(sales_with_product_txt, conn))}\")\nprint(pd.read_sql(sales_with_product_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(sales_with_product_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Sales with Product Information: 100\n</code></pre> <pre><code>                  date  customer_id  product_id     category  sales_amount  quantity  product_name   price\n0  2023-01-01 00:00:00           52          45         Food        490.76         7    Product 45  493.14\n1  2023-01-02 00:00:00           93          41  Electronics        453.94         5    Product 41  193.39\n2  2023-01-03 00:00:00           15          29         Home        994.51         5    Product 29   80.07\n3  2023-01-04 00:00:00           72          15  Electronics        184.17         7    Product 15  153.67\n4  2023-01-05 00:00:00           61          45         Food         27.89         9    Product 45  493.14\n</code></pre> date customer_id product_id category sales_amount quantity product_name price 0 2023-01-01 00:00:00 52 45 Food 490.76 7 Product 45 493.14 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 Product 41 193.39 2 2023-01-03 00:00:00 15 29 Home 994.51 5 Product 29 80.07 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 Product 15 153.67 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Product 45 493.14 <p>In PySpark, we can use the <code>.join()</code> method to combine rows from two or more DataFrames based on a related column between them. In this case, we will join the <code>sales</code> DataFrame with the <code>product</code> DataFrame on the <code>product_id</code> column.</p> Join sales with product data<pre><code>sales_with_product_ps: psDataFrame = df_sales_ps.join(\n    other=df_product_ps.select(\"product_id\", \"product_name\", \"price\"),\n    on=\"product_id\",\n    how=\"left\",\n)\nprint(f\"Sales with Product Information: {sales_with_product_ps.count()}\")\nsales_with_product_ps.show(5)\nprint(sales_with_product_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Sales with Product Information: 100\n</code></pre> <pre><code>+----------+-------------------+-----------+-----------+------------+--------+------------+------+\n|product_id|               date|customer_id|   category|sales_amount|quantity|product_name| price|\n+----------+-------------------+-----------+-----------+------------+--------+------------+------+\n|         1|2023-01-06 00:00:00|         21|   Clothing|      498.95|       5|   Product 1|257.57|\n|         1|2023-01-11 00:00:00|         88|Electronics|      314.98|       9|   Product 1|257.57|\n|         1|2023-02-11 00:00:00|         55|       Food|       199.0|       5|   Product 1|257.57|\n|         1|2023-04-04 00:00:00|         85|       Food|      146.97|       7|   Product 1|257.57|\n|         5|2023-01-21 00:00:00|         64|Electronics|      356.58|       5|   Product 5|200.71|\n+----------+-------------------+-----------+-----------+------------+--------+------------+------+\nonly showing top 5 rows\n</code></pre> product_id date customer_id category sales_amount quantity product_name price 0 1 2023-01-11 00:00:00 88 Electronics 314.98 9 Product 1 257.57 1 1 2023-02-11 00:00:00 55 Food 199 5 Product 1 257.57 2 5 2023-01-21 00:00:00 64 Electronics 356.58 5 Product 5 200.71 3 5 2023-02-18 00:00:00 39 Books 79.71 8 Product 5 200.71 4 6 2023-03-23 00:00:00 34 Electronics 48.45 8 Product 6 15.31 <p>In Polars, we can use the <code>.join()</code> method to combine rows from two or more DataFrames based on a related column between them. In this case, we will join the <code>sales</code> DataFrame with the <code>product</code> DataFrame on the <code>product_id</code> column.</p> Join sales with product data<pre><code>sales_with_product_pl: pl.DataFrame = df_sales_pl.join(\n    df_product_pl.select([\"product_id\", \"product_name\", \"price\"]),\n    on=\"product_id\",\n    how=\"left\",\n)\nprint(f\"Sales with Product Information: {len(sales_with_product_pl)}\")\nprint(sales_with_product_pl.head(5))\nprint(sales_with_product_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Sales with Product Information: 100\n</code></pre> <pre><code>shape: (5, 8)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date                \u2506 customer_id \u2506 product_id \u2506 category    \u2506 sales_amount \u2506 quantity \u2506 product_name \u2506 price  \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---        \u2506 ---         \u2506 ---          \u2506 ---      \u2506 ---          \u2506 ---    \u2502\n\u2502 datetime[ns]        \u2506 i64         \u2506 i64        \u2506 str         \u2506 f64          \u2506 i64      \u2506 str          \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 00:00:00 \u2506 52          \u2506 45         \u2506 Food        \u2506 490.76       \u2506 7        \u2506 Product 45   \u2506 493.14 \u2502\n\u2502 2023-01-02 00:00:00 \u2506 93          \u2506 41         \u2506 Electronics \u2506 453.94       \u2506 5        \u2506 Product 41   \u2506 193.39 \u2502\n\u2502 2023-01-03 00:00:00 \u2506 15          \u2506 29         \u2506 Home        \u2506 994.51       \u2506 5        \u2506 Product 29   \u2506 80.07  \u2502\n\u2502 2023-01-04 00:00:00 \u2506 72          \u2506 15         \u2506 Electronics \u2506 184.17       \u2506 7        \u2506 Product 15   \u2506 153.67 \u2502\n\u2502 2023-01-05 00:00:00 \u2506 61          \u2506 45         \u2506 Food        \u2506 27.89        \u2506 9        \u2506 Product 45   \u2506 493.14 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date customer_id product_id category sales_amount quantity product_name price 0 2023-01-01 00:00:00 52 45 Food 490.76 7 Product 45 493.14 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 Product 41 193.39 2 2023-01-03 00:00:00 15 29 Home 994.51 5 Product 29 80.07 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 Product 15 153.67 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Product 45 493.14 <p>In the next step, we will join the resulting DataFrame with the <code>customer</code> DataFrame to get customer information for each sale. This allows us to create a complete view of the sales data, including product and customer details.</p> PandasSQLPySparkPolars <p>This process is similar to the previous step, but now we will extend the <code>sales_with_product</code> DataFrame to join it with the <code>customer</code> DataFrame on the <code>customer_id</code> column. This will give us a complete view of the sales data, including product and customer details.</p> Join with customer information to get a complete view<pre><code>complete_sales_pd: pd.DataFrame = pd.merge(\n    sales_with_product_pd,\n    df_customer_pd[[\"customer_id\", \"customer_name\", \"city\", \"state\"]],\n    on=\"customer_id\",\n    how=\"left\",\n)\nprint(f\"Complete Sales Data with Customer Information: {len(complete_sales_pd)}\")\nprint(complete_sales_pd.head(5))\nprint(complete_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Complete Sales Data with Customer Information: 100\n</code></pre> <pre><code>        date  customer_id  product_id     category  sales_amount  quantity  product_name   price  customer_name      city  state\n0 2023-01-01           52          45         Food        490.76         7    Product 45  493.14    Customer 52   Phoenix     TX\n1 2023-01-02           93          41  Electronics        453.94         5    Product 41  193.39    Customer 93  New York     TX\n2 2023-01-03           15          29         Home        994.51         5    Product 29   80.07    Customer 15  New York     CA\n3 2023-01-04           72          15  Electronics        184.17         7    Product 15  153.67    Customer 72   Houston     IL\n4 2023-01-05           61          45         Food         27.89         9    Product 45  493.14    Customer 61   Phoenix     IL\n</code></pre> date customer_id product_id category sales_amount quantity product_name price customer_name city state 0 2023-01-01 00:00:00 52 45 Food 490.76 7 Product 45 493.14 Customer 52 Phoenix TX 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 Product 41 193.39 Customer 93 New York TX 2 2023-01-03 00:00:00 15 29 Home 994.51 5 Product 29 80.07 Customer 15 New York CA 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 Product 15 153.67 Customer 72 Houston IL 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Product 45 493.14 Customer 61 Phoenix IL <p>This process is similar to the previous step, but now we will extend the <code>sales_with_product</code> DataFrame to join it with the <code>customer</code> DataFrame on the <code>customer_id</code> column. This will give us a complete view of the sales data, including product and customer details.</p> Join with customer information to get a complete view<pre><code>complete_sales_txt: str = \"\"\"\n    SELECT\n        s.*,\n        p.product_name,\n        p.price,\n        c.customer_name,\n        c.city,\n        c.state\n    FROM sales s\n    LEFT JOIN product p ON s.product_id = p.product_id\n    LEFT JOIN customer c ON s.customer_id = c.customer_id\n\"\"\"\nprint(f\"Complete Sales Data with Customer Information: {len(pd.read_sql(complete_sales_txt, conn))}\")\nprint(pd.read_sql(complete_sales_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(complete_sales_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Complete Sales Data with Customer Information: 100\n</code></pre> <pre><code>                  date  customer_id  product_id     category  sales_amount  quantity  product_name   price  customer_name      city  state\n0  2023-01-01 00:00:00           52          45         Food        490.76         7    Product 45  493.14    Customer 52   Phoenix     TX\n1  2023-01-02 00:00:00           93          41  Electronics        453.94         5    Product 41  193.39    Customer 93  New York     TX\n2  2023-01-03 00:00:00           15          29         Home        994.51         5    Product 29   80.07    Customer 15  New York     CA\n3  2023-01-04 00:00:00           72          15  Electronics        184.17         7    Product 15  153.67    Customer 72   Houston     IL\n4  2023-01-05 00:00:00           61          45         Food         27.89         9    Product 45  493.14    Customer 61   Phoenix     IL\n</code></pre> date customer_id product_id category sales_amount quantity product_name price customer_name city state 0 2023-01-01 00:00:00 52 45 Food 490.76 7 Product 45 493.14 Customer 52 Phoenix TX 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 Product 41 193.39 Customer 93 New York TX 2 2023-01-03 00:00:00 15 29 Home 994.51 5 Product 29 80.07 Customer 15 New York CA 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 Product 15 153.67 Customer 72 Houston IL 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Product 45 493.14 Customer 61 Phoenix IL <p>This process is similar to the previous step, but now we will extend the <code>sales_with_product</code> DataFrame to join it with the <code>customer</code> DataFrame on the <code>customer_id</code> column. This will give us a complete view of the sales data, including product and customer details.</p> Join with customer information to get a complete view<pre><code>complete_sales_ps: psDataFrame = sales_with_product_ps.alias(\"s\").join(\n    other=df_customer_ps.select(\"customer_id\", \"customer_name\", \"city\", \"state\").alias(\"c\"),\n    on=\"customer_id\",\n    how=\"left\",\n)\nprint(f\"Complete Sales Data with Customer Information: {complete_sales_ps.count()}\")\ncomplete_sales_ps.show(5)\nprint(complete_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Complete Sales Data with Customer Information: 100\n</code></pre> <pre><code>+-----------+----------+-------------------+-----------+------------+--------+------------+------+-------------+-----------+-----+\n|customer_id|product_id|               date|   category|sales_amount|quantity|product_name| price|customer_name|       city|state|\n+-----------+----------+-------------------+-----------+------------+--------+------------+------+-------------+-----------+-----+\n|         39|         5|2023-02-18 00:00:00|      Books|       79.71|       8|   Product 5|200.71|  Customer 39|Los Angeles|   NY|\n|         88|         1|2023-01-11 00:00:00|Electronics|      314.98|       9|   Product 1|257.57|  Customer 88|Los Angeles|   TX|\n|         85|         1|2023-04-04 00:00:00|       Food|      146.97|       7|   Product 1|257.57|  Customer 85|    Phoenix|   CA|\n|         55|         1|2023-02-11 00:00:00|       Food|       199.0|       5|   Product 1|257.57|  Customer 55|Los Angeles|   NY|\n|         21|         1|2023-01-06 00:00:00|   Clothing|      498.95|       5|   Product 1|257.57|  Customer 21|Los Angeles|   IL|\n+-----------+----------+-------------------+-----------+------------+--------+------------+------+-------------+-----------+-----+\nonly showing top 5 rows\n</code></pre> customer_id product_id date category sales_amount quantity product_name price customer_name city state 0 88 1 2023-01-11 00:00:00 Electronics 314.98 9 Product 1 257.57 Customer 88 Los Angeles TX 1 55 1 2023-02-11 00:00:00 Food 199 5 Product 1 257.57 Customer 55 Los Angeles NY 2 64 5 2023-01-21 00:00:00 Electronics 356.58 5 Product 5 200.71 Customer 64 Los Angeles NY 3 39 5 2023-02-18 00:00:00 Books 79.71 8 Product 5 200.71 Customer 39 Los Angeles NY 4 34 6 2023-03-23 00:00:00 Electronics 48.45 8 Product 6 15.31 Customer 34 Los Angeles NY <p>This process is similar to the previous step, but now we will extend the <code>sales_with_product</code> DataFrame to join it with the <code>customer</code> DataFrame on the <code>customer_id</code> column. This will give us a complete view of the sales data, including product and customer details.</p> Join with customer information to get a complete view<pre><code>complete_sales_pl: pl.DataFrame = sales_with_product_pl.join(\n    df_customer_pl.select([\"customer_id\", \"customer_name\", \"city\", \"state\"]),\n    on=\"customer_id\",\n    how=\"left\",\n)\nprint(f\"Complete Sales Data with Customer Information: {len(complete_sales_pl)}\")\nprint(complete_sales_pl.head(5))\nprint(complete_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Complete Sales Data with Customer Information: 100\n</code></pre> <pre><code>shape: (5, 11)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date                \u2506 customer_id \u2506 product_id \u2506 category    \u2506 sales_amount \u2506 quantity \u2506 product_name \u2506 price  \u2506 customer_name \u2506 city     \u2506 state \u2502\n\u2502 ---                 \u2506 ---         \u2506 ---        \u2506 ---         \u2506 ---          \u2506 ---      \u2506 ---          \u2506 ---    \u2506 ---           \u2506 ---      \u2506 ---   \u2502\n\u2502 datetime[ns]        \u2506 i64         \u2506 i64        \u2506 str         \u2506 f64          \u2506 i64      \u2506 str          \u2506 f64    \u2506 str           \u2506 str      \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 00:00:00 \u2506 52          \u2506 45         \u2506 Food        \u2506 490.76       \u2506 7        \u2506 Product 45   \u2506 493.14 \u2506 Customer 52   \u2506 Phoenis  \u2506 TX    \u2502\n\u2502 2023-01-02 00:00:00 \u2506 93          \u2506 41         \u2506 Electronics \u2506 453.94       \u2506 5        \u2506 Product 41   \u2506 193.39 \u2506 Customer 93   \u2506 New York \u2506 TX    \u2502\n\u2502 2023-01-03 00:00:00 \u2506 15          \u2506 29         \u2506 Home        \u2506 994.51       \u2506 5        \u2506 Product 29   \u2506 80.07  \u2506 Customer 15   \u2506 New York \u2506 CA    \u2502\n\u2502 2023-01-04 00:00:00 \u2506 72          \u2506 15         \u2506 Electronics \u2506 184.17       \u2506 7        \u2506 Product 15   \u2506 153.67 \u2506 Customer 72   \u2506 Houston  \u2506 IL    \u2502\n\u2502 2023-01-05 00:00:00 \u2506 61          \u2506 45         \u2506 Food        \u2506 27.89        \u2506 9        \u2506 Product 45   \u2506 493.14 \u2506 Customer 61   \u2506 Phoenix  \u2506 IL    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date customer_id product_id category sales_amount quantity product_name price customer_name city state 0 2023-01-01 00:00:00 52 45 Food 490.76 7 Product 45 493.14 Customer 52 Phoenix TX 1 2023-01-02 00:00:00 93 41 Electronics 453.94 5 Product 41 193.39 Customer 93 New York TX 2 2023-01-03 00:00:00 15 29 Home 994.51 5 Product 29 80.07 Customer 15 New York CA 3 2023-01-04 00:00:00 72 15 Electronics 184.17 7 Product 15 153.67 Customer 72 Houston IL 4 2023-01-05 00:00:00 61 45 Food 27.89 9 Product 45 493.14 Customer 61 Phoenix IL <p>Once we have the complete sales data, we can calculate the revenue for each sale by multiplying the price and quantity (columns from different tables). We can also compare this calculated revenue with the sales amount to identify any discrepancies.</p> PandasSQLPySparkPolars <p>In Pandas, we can calculate the revenue for each sale by multiplying the <code>price</code> and <code>quantity</code> columns. We can then compare this calculated revenue with the <code>sales_amount</code> column to identify any discrepancies.</p> <p>Notice here that the syntax for Pandas uses the <code>DataFrame</code> object directly, and we can access the columns using the 'slice' (<code>[]</code>) operator.</p> Calculate revenue and compare with sales amount<pre><code>complete_sales_pd[\"calculated_revenue\"] = complete_sales_pd[\"price\"] * complete_sales_pd[\"quantity\"]\ncomplete_sales_pd[\"price_difference\"] = complete_sales_pd[\"sales_amount\"] - complete_sales_pd[\"calculated_revenue\"]\nprint(f\"Complete Sales Data with Calculated Revenue and Price Difference: {len(complete_sales_pd)}\")\nprint(complete_sales_pd[[\"sales_amount\", \"price\", \"quantity\", \"calculated_revenue\", \"price_difference\"]].head(5))\nprint(\n    complete_sales_pd[[\"sales_amount\", \"price\", \"quantity\", \"calculated_revenue\", \"price_difference\"]]\n    .head(5)\n    .to_markdown()\n)\n</code></pre> <pre><code>Complete Sales Data with Calculated Revenue and Price Difference: 100\n</code></pre> <pre><code>   sales_amount   price  quantity  calculated_revenue  price_difference\n0        490.76  493.14         7             3451.98          -2961.22\n1        453.94  193.39         5              966.95           -513.01\n2        994.51   80.07         5              400.35            594.16\n3        184.17  153.67         7             1075.69           -891.52\n4         27.89  493.14         9             4438.26          -4410.37\n</code></pre> sales_amount price quantity calculated_revenue price_difference 0 490.76 493.14 7 3451.98 -2961.22 1 453.94 193.39 5 966.95 -513.01 2 994.51 80.07 5 400.35 594.16 3 184.17 153.67 7 1075.69 -891.52 4 27.89 493.14 9 4438.26 -4410.37 <p>In SQL, we can calculate the revenue for each sale by multiplying the <code>price</code> and <code>quantity</code> columns. We can then compare this calculated revenue with the <code>sales_amount</code> column to identify any discrepancies.</p> Calculate revenue and compare with sales amount<pre><code>revenue_comparison_txt: str = \"\"\"\n    SELECT\n        s.sales_amount,\n        p.price,\n        s.quantity,\n        (p.price * s.quantity) AS calculated_revenue,\n        (s.sales_amount - (p.price * s.quantity)) AS price_difference\n    FROM sales s\n    LEFT JOIN product p ON s.product_id = p.product_id\n\"\"\"\nprint(\n    f\"Complete Sales Data with Calculated Revenue and Price Difference: {len(pd.read_sql(revenue_comparison_txt, conn))}\"\n)\nprint(pd.read_sql(revenue_comparison_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(revenue_comparison_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Complete Sales Data with Calculated Revenue and Price Difference: 100\n</code></pre> <pre><code>   sales_amount   price  quantity  calculated_revenue  price_difference\n0        490.76  493.14         7             3451.98          -2961.22\n1        453.94  193.39         5              966.95           -513.01\n2        994.51   80.07         5              400.35            594.16\n3        184.17  153.67         7             1075.69           -891.52\n4         27.89  493.14         9             4438.26          -4410.37\n</code></pre> sales_amount price quantity calculated_revenue price_difference 0 490.76 493.14 7 3451.98 -2961.22 1 453.94 193.39 5 966.95 -513.01 2 994.51 80.07 5 400.35 594.16 3 184.17 153.67 7 1075.69 -891.52 4 27.89 493.14 9 4438.26 -4410.37 <p>In PySpark, we can calculate the revenue for each sale by multiplying the <code>price</code> and <code>quantity</code> columns. We can then compare this calculated revenue with the <code>sales_amount</code> column to identify any discrepancies.</p> <p>Notice here that the syntax for PySpark uses the <code>.withColumns</code> method to add new multiple columns to the DataFrame simultaneously. This method takes a dictionary where the keys are the names of the new columns and the values are the expressions to compute those columns. The methematical computation we have shown here uses two different methods:</p> <ol> <li>With the PySpark API, we can use the <code>F.col()</code> function to refer to the columns, and multiply them directly</li> <li>With the Spark SQL API, we can use the <code>F.expr()</code> function to write a SQL-like expression for the calculation.</li> </ol> Calculate revenue and compare with sales amount<pre><code>complete_sales_ps: psDataFrame = complete_sales_ps.withColumns(\n    {\n        \"calculated_revenue\": F.col(\"price\") * F.col(\"quantity\"),\n        \"price_difference\": F.expr(\"sales_amount - (price * quantity)\"),\n    },\n).select(\"sales_amount\", \"price\", \"quantity\", \"calculated_revenue\", \"price_difference\")\nprint(f\"Complete Sales Data with Calculated Revenue and Price Difference: {complete_sales_ps.count()}\")\ncomplete_sales_ps.show(5)\nprint(complete_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Complete Sales Data with Calculated Revenue and Price Difference: 100\n</code></pre> <pre><code>+------------+------+--------+------------------+------------------+\n|sales_amount| price|quantity|calculated_revenue|  price_difference|\n+------------+------+--------+------------------+------------------+\n|       79.71|200.71|       8|           1605.68|          -1525.97|\n|      314.98|257.57|       9|           2318.13|          -2003.15|\n|      146.97|257.57|       7|           1802.99|          -1656.02|\n|       199.0|257.57|       5|           1287.85|          -1088.85|\n|      498.95|257.57|       5|           1287.85|-788.8999999999999|\n+------------+------+--------+------------------+------------------+\nonly showing top 5 rows\n</code></pre> sales_amount price quantity calculated_revenue price_difference 0 48.45 15.31 8 122.48 -74.03 1 79.71 200.71 8 1605.68 -1525.97 2 314.98 257.57 9 2318.13 -2003.15 3 199 257.57 5 1287.85 -1088.85 4 356.58 200.71 5 1003.55 -646.97 <p>In Polars, we can calculate the revenue for each sale by multiplying the <code>price</code> and <code>quantity</code> columns. We can then compare this calculated revenue with the <code>sales_amount</code> column to identify any discrepancies.</p> <p>Notice here that the syntax for Polars uses the <code>.with_columns</code> method to add new multiple columns to the DataFrame simultaneously. This method takes a list of expressions, where each expression defines a new column.</p> Calculate revenue and compare with sales amount<pre><code>complete_sales_pl: pl.DataFrame = complete_sales_pl.with_columns(\n    (pl.col(\"price\") * pl.col(\"quantity\")).alias(\"calculated_revenue\"),\n    (pl.col(\"sales_amount\") - (pl.col(\"price\") * pl.col(\"quantity\"))).alias(\"price_difference\"),\n)\nprint(f\"Complete Sales Data with Calculated Revenue and Price Difference: {len(complete_sales_pl)}\")\nprint(complete_sales_pl.select([\"sales_amount\", \"price\", \"quantity\", \"calculated_revenue\", \"price_difference\"]).head(5))\nprint(\n    complete_sales_pl.select([\"sales_amount\", \"price\", \"quantity\", \"calculated_revenue\", \"price_difference\"])\n    .head(5)\n    .to_pandas()\n    .to_markdown()\n)\n</code></pre> <pre><code>Complete Sales Data with Calculated Revenue and Price Difference: 100\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sales_amount \u2506 price  \u2506 quantity \u2506 calculated_revenue \u2506 price_difference \u2502\n\u2502 ---          \u2506 ---    \u2506 ---      \u2506 ---                \u2506 ---              \u2502\n\u2502 f64          \u2506 f64    \u2506 i64      \u2506 f64                \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 490.76       \u2506 493.14 \u2506 7        \u2506 3451.98            \u2506 -2961.22         \u2502\n\u2502 453.94       \u2506 193.39 \u2506 5        \u2506 966.95             \u2506 -513.01          \u2502\n\u2502 994.51       \u2506 80.07  \u2506 5        \u2506 400.35             \u2506 594.16           \u2502\n\u2502 184.17       \u2506 153.67 \u2506 7        \u2506 1075.69            \u2506 -891.52          \u2502\n\u2502 27.89        \u2506 493.14 \u2506 9        \u2506 4438.26            \u2506 -4410.37         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> sales_amount price quantity calculated_revenue price_difference 0 490.76 493.14 7 3451.98 -2961.22 1 453.94 193.39 5 966.95 -513.01 2 994.51 80.07 5 400.35 594.16 3 184.17 153.67 7 1075.69 -891.52 4 27.89 493.14 9 4438.26 -4410.37"},{"location":"guides/querying-data/#4-window-functions","title":"4. Window Functions","text":"<p>Window functions are a powerful feature in Pandas that allow us to perform calculations across a set of rows related to the current row. This is particularly useful for time series data, where we may want to calculate rolling averages, cumulative sums, or other metrics based on previous or subsequent rows.</p> <p>To understand more about the nuances of the window functions, check out some of these guides:</p> <ul> <li>Analyzing data with window functions</li> <li>SQL Window Functions Visualized</li> </ul> <p>In this section, we will demonstrate how to use window functions to analyze sales data over time. We will start by converting the <code>date</code> column to a datetime type, which is necessary for time-based calculations. We will then group the data by date and calculate the total sales for each day.</p> <p>The first thing that we will do is to group the sales data by date and calculate the total sales for each day. This will give us a daily summary of sales, which we can then use to analyze trends over time.</p> PandasSQLPySparkPolars <p>In Pandas, we can use the <code>.groupby()</code> method to group the data by the <code>date</code> column, followed by the <code>.agg()</code> method to calculate the total sales for each day. This will then set us up for further time-based calculations in the following steps</p> Time-based window function<pre><code>df_sales_pd[\"date\"] = pd.to_datetime(df_sales_pd[\"date\"])  # Ensure correct date type\ndaily_sales_pd: pd.DataFrame = (\n    df_sales_pd.groupby(df_sales_pd[\"date\"].dt.date)\n    .agg(total_sales=(\"sales_amount\", \"sum\"))\n    .reset_index()\n    .sort_values(\"date\")\n)\nprint(f\"Daily Sales Summary: {len(daily_sales_pd)}\")\nprint(daily_sales_pd.head(5))\nprint(daily_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Daily Sales Summary: 100\n</code></pre> <pre><code>         date  total_sales\n0  2023-01-01        490.76\n1  2023-01-02        453.94\n2  2023-01-03        994.51\n3  2023-01-04        184.17\n4  2023-01-05         27.89\n</code></pre> date total_sales 0 2023-01-01 490.76 1 2023-01-02 453.94 2 2023-01-03 994.51 3 2023-01-04 184.17 4 2023-01-05 27.89 <p>In SQL, we can use the <code>GROUP BY</code> clause to group the data by the <code>date</code> column and then use the <code>SUM()</code> function to calculate the total sales for each day. This will give us a daily summary of sales, which we can then use to analyze trends over time.</p> Time-based window function<pre><code>daily_sales_txt: str = \"\"\"\n    SELECT\n        date,\n        SUM(sales_amount) AS total_sales\n    FROM sales\n    GROUP BY date\n    ORDER BY date\n\"\"\"\nprint(f\"Daily Sales Summary: {len(pd.read_sql(daily_sales_txt, conn))}\")\nprint(pd.read_sql(daily_sales_txt + \"LIMIT 5\", conn))\nprint(pd.read_sql(daily_sales_txt + \"LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Daily Sales Summary: 100\n</code></pre> <pre><code>                  date  total_sales\n0  2023-01-01 00:00:00       490.76\n1  2023-01-02 00:00:00       453.94\n2  2023-01-03 00:00:00       994.51\n3  2023-01-04 00:00:00       184.17\n4  2023-01-05 00:00:00        27.89\n</code></pre> date total_sales 0 2023-01-01 00:00:00 490.76 1 2023-01-02 00:00:00 453.94 2 2023-01-03 00:00:00 994.51 3 2023-01-04 00:00:00 184.17 4 2023-01-05 00:00:00 27.89 <p>In PySpark, we can use the <code>.groupBy()</code> method to group the data by the <code>date</code> column, followed by the <code>.agg()</code> method to calculate the total sales for each day. This will then set us up for further time-based calculations in the following steps.</p> Time-based window function<pre><code>df_sales_ps: psDataFrame = df_sales_ps.withColumn(\"date\", F.to_date(df_sales_ps[\"date\"]))\ndaily_sales_ps: psDataFrame = (\n    df_sales_ps.groupBy(\"date\").agg(F.sum(\"sales_amount\").alias(\"total_sales\")).orderBy(\"date\")\n)\nprint(f\"Daily Sales Summary: {daily_sales_ps.count()}\")\ndaily_sales_ps.show(5)\nprint(daily_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Daily Sales Summary: 100\n</code></pre> <pre><code>+----------+-----------+\n|      date|total_sales|\n+----------+-----------+\n|2023-01-01|     490.76|\n|2023-01-02|     453.94|\n|2023-01-03|     994.51|\n|2023-01-04|     184.17|\n|2023-01-05|      27.89|\n+----------+-----------+\nonly showing top 5 rows\n</code></pre> date total_sales 0 2023-01-01 490.76 1 2023-01-02 453.94 2 2023-01-03 994.51 3 2023-01-04 184.17 4 2023-01-05 27.89 <p>In Polars, we can use the <code>.group_by()</code> method to group the data by the <code>date</code> column, followed by the <code>.agg()</code> method to calculate the total sales for each day. This will then set us up for further time-based calculations in the following steps.</p> Time-based window function<pre><code>df_sales_pl: pl.DataFrame = df_sales_pl.with_columns(pl.col(\"date\").cast(pl.Date))\ndaily_sales_pl: pl.DataFrame = (\n    df_sales_pl.group_by(\"date\").agg(pl.col(\"sales_amount\").sum().alias(\"total_sales\")).sort(\"date\")\n)\nprint(f\"Daily Sales Summary: {len(daily_sales_pl)}\")\nprint(daily_sales_pl.head(5))\nprint(daily_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Daily Sales Summary: 100\n</code></pre> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date       \u2506 total_sales \u2502\n\u2502 ---        \u2506 ---         \u2502\n\u2502 date       \u2506 f64         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 \u2506 490.76      \u2502\n\u2502 2023-01-02 \u2506 453.94      \u2502\n\u2502 2023-01-03 \u2506 994.51      \u2502\n\u2502 2023-01-04 \u2506 184.17      \u2502\n\u2502 2023-01-05 \u2506 27.89       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date total_sales 0 2023-01-01 00:00:00 490.76 1 2023-01-02 00:00:00 453.94 2 2023-01-03 00:00:00 994.51 3 2023-01-04 00:00:00 184.17 4 2023-01-05 00:00:00 27.89 <p>Next, we will calculate the lag and lead values for the sales amount. This allows us to compare the current day's sales with the previous and next days' sales.</p> PandasSQLPySparkPolars <p>In Pandas, we can calculate the lag and lead values for the sales amount by using the <code>.shift()</code> method. This method shifts the values in a column by a specified number of periods, allowing us to create lag and lead columns.</p> <p>Note that the <code>.shift()</code> method simply shifts the values in the column by a number of rows up or down, so we can use it to create lag and lead columns. This function itself does not need to be ordered because it assumes that the DataFrame is already ordered. However, if you want it to be ordered, you can use the <code>.sort_values()</code> method before applying <code>.shift()</code>.</p> Calculate lag and lead<pre><code>daily_sales_pd[\"previous_day_sales\"] = daily_sales_pd[\"total_sales\"].shift(1)\ndaily_sales_pd[\"next_day_sales\"] = daily_sales_pd[\"total_sales\"].shift(-1)\nprint(f\"Daily Sales with Lag and Lead: {len(daily_sales_pd)}\")\nprint(daily_sales_pd.head(5))\nprint(daily_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Daily Sales with Lag and Lead: 100\n</code></pre> <pre><code>         date  total_sales  previous_day_sales  next_day_sales\n0  2023-01-01       490.76                 NaN          453.94\n1  2023-01-02       453.94              490.76          994.51\n2  2023-01-03       994.51              453.94          184.17\n3  2023-01-04       184.17              994.51           27.89\n4  2023-01-05        27.89              184.17          498.95\n</code></pre> date total_sales previous_day_sales next_day_sales 0 2023-01-01 490.76 nan 453.94 1 2023-01-02 453.94 490.76 994.51 2 2023-01-03 994.51 453.94 184.17 3 2023-01-04 184.17 994.51 27.89 4 2023-01-05 27.89 184.17 498.95 <p>In SQL, we can use the <code>LAG()</code> and <code>LEAD()</code> window functions to calculate the lag and lead values for the sales amount. These functions allow us to access data from previous and next rows in the result set without needing to join the table to itself.</p> <p>The part that is important to note here is that the <code>LAG()</code> and <code>LEAD()</code> functions are used in conjunction with the <code>OVER</code> clause, which defines the window over which the function operates. In this case, we are ordering by the <code>date</code> column to ensure that the lag and lead values are calculated based on the chronological order of the sales data.</p> Calculate lag and lead<pre><code>lag_lead_txt: str = \"\"\"\n    SELECT\n        date AS sale_date,\n        SUM(sales_amount) AS total_sales,\n        LAG(SUM(sales_amount)) OVER (ORDER BY date) AS previous_day_sales,\n        LEAD(SUM(sales_amount)) OVER (ORDER BY date) AS next_day_sales\n    FROM sales\n    GROUP BY date\n    ORDER BY date\n\"\"\"\nlag_lead_df_sql: pd.DataFrame = pd.read_sql(lag_lead_txt, conn)\nprint(f\"Daily Sales with Lag and Lead: {len(lag_lead_df_sql)}\")\nprint(pd.read_sql(lag_lead_txt + \" LIMIT 5\", conn))\nprint(pd.read_sql(lag_lead_txt + \" LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Daily Sales with Lag and Lead: 100\n</code></pre> <pre><code>             sale_date  total_sales  previous_day_sales  next_day_sales\n0  2023-01-01 00:00:00        490.76                 NaN          453.94\n1  2023-01-02 00:00:00        453.94              490.76          994.51\n2  2023-01-03 00:00:00        994.51              453.94          184.17\n3  2023-01-04 00:00:00        184.17              994.51           27.89\n4  2023-01-05 00:00:00         27.89              184.17          498.95\n</code></pre> sale_date total_sales previous_day_sales next_day_sales 0 2023-01-01 00:00:00 490.76 nan 453.94 1 2023-01-02 00:00:00 453.94 490.76 994.51 2 2023-01-03 00:00:00 994.51 453.94 184.17 3 2023-01-04 00:00:00 184.17 994.51 27.89 4 2023-01-05 00:00:00 27.89 184.17 498.95 <p>In PySpark, we can use the <code>.lag()</code> and <code>.lead()</code> functions to calculate the lag and lead values for the sales amount. These functions are used in conjunction with a window specification that defines the order of the rows.</p> <p>Note that in PySpark, we can define a Window function in one of two ways: using the PySpark API or using the Spark SQL API.</p> <ol> <li>The PySpark API: The PySpark API allows us to define a window specification using the <code>Window()</code> class, which provides methods to specify the ordering of the rows. We can then use the <code>F.lag()</code> and <code>F.lead()</code> functions to calculate the lag and lead values over a given window on the table.</li> <li>The Spark SQL API: The Spark SQL API is used through the <code>F.expr()</code> function, which allows us to write SQL-like expressions for the calculations. This is similar to how we would write SQL queries, but it is executed within the PySpark context.</li> </ol> <p>Here in the below example, we show how the previous day sales can be calculated using the <code>.lag()</code> function in the PySpark API, and the next day sales can be calculated using the <code>LEAD()</code> function in the Spark SQL API. Functionally, both of these two methods achieve the same result, but aesthetically they use slightly different syntax. It is primarily a matter of preference which one you choose to use.</p> Calculate lag and lead<pre><code>window_spec_ps: Window = Window.orderBy(\"date\")\ndaily_sales_ps: psDataFrame = daily_sales_ps.withColumns(\n    {\n        \"previous_day_sales\": F.lag(\"total_sales\").over(window_spec_ps),\n        \"next_day_sales\": F.expr(\"LEAD(total_sales) OVER (ORDER BY date)\"),\n    },\n)\nprint(f\"Daily Sales with Lag and Lead: {daily_sales_ps.count()}\")\ndaily_sales_ps.show(5)\nprint(daily_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Daily Sales with Lag and Lead: 100\n</code></pre> <pre><code>+----------+-----------+------------------+--------------+\n|      date|total_sales|previous_day_sales|next_day_sales|\n+----------+-----------+------------------+--------------+\n|2023-01-01|     490.76|              NULL|        453.94|\n|2023-01-02|     453.94|            490.76|        994.51|\n|2023-01-03|     994.51|            453.94|        184.17|\n|2023-01-04|     184.17|            994.51|         27.89|\n|2023-01-05|      27.89|            184.17|        498.95|\n+----------+-----------+------------------+--------------+\nonly showing top 5 rows\n</code></pre> date total_sales previous_day_sales next_day_sales 0 2023-01-01 490.76 nan 453.94 1 2023-01-02 453.94 490.76 994.51 2 2023-01-03 994.51 453.94 184.17 3 2023-01-04 184.17 994.51 27.89 4 2023-01-05 27.89 184.17 498.95 <p>In Polars, we can use the <code>.shift()</code> method to calculate the lag and lead values for the sales amount. This method shifts the values in a column by a specified number of periods, allowing us to create lag and lead columns.</p> <p>Note that the <code>.shift()</code> method simply shifts the values in the column by a number of rows up or down, so we can use it to create lag and lead columns. This function itself does not need to be ordered because it assumes that the DataFrame is already ordered. However, if you want it to be ordered, you can use the <code>.sort()</code> method before applying <code>.shift()</code>.</p> Calculate lag and lead<pre><code>daily_sales_pl: pl.DataFrame = daily_sales_pl.with_columns(\n    pl.col(\"total_sales\").shift(1).alias(\"previous_day_sales\"),\n    pl.col(\"total_sales\").shift(-1).alias(\"next_day_sales\"),\n)\nprint(f\"Daily Sales with Lag and Lead: {len(daily_sales_pl)}\")\nprint(daily_sales_pl.head(5))\nprint(daily_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Daily Sales with Lag and Lead: 100\n</code></pre> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date       \u2506 total_sales \u2506 previous_day_sales \u2506 next_day_sales \u2502\n\u2502 ---        \u2506 ---         \u2506 ---                \u2506 ---            \u2502\n\u2502 date       \u2506 f64         \u2506 f64                \u2506 f64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 \u2506 490.76      \u2506 null               \u2506 453.94         \u2502\n\u2502 2023-01-02 \u2506 453.94      \u2506 490.76             \u2506 994.51         \u2502\n\u2502 2023-01-03 \u2506 994.51      \u2506 453.94             \u2506 184.17         \u2502\n\u2502 2023-01-04 \u2506 184.17      \u2506 994.51             \u2506 27.89          \u2502\n\u2502 2023-01-05 \u2506 27.89       \u2506 184.17             \u2506 498.95         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date total_sales previous_day_sales next_day_sales 0 2023-01-01 00:00:00 490.76 nan 453.94 1 2023-01-02 00:00:00 453.94 490.76 994.51 2 2023-01-03 00:00:00 994.51 453.94 184.17 3 2023-01-04 00:00:00 184.17 994.51 27.89 4 2023-01-05 00:00:00 27.89 184.17 498.95 <p>Now, we can calculate the day-over-day change in sales. This is done by subtracting the previous day's sales from the current day's sales. Then secondly, we can calculate the percentage change in sales using the formula:</p> <pre><code>((current_day_sales - previous_day_sales) / previous_day_sales) * 100\n</code></pre> PandasSQLPySparkPolars <p>In Pandas, we can calculate the day-over-day change in sales by subtracting the <code>previous_day_sales</code> column from the <code>total_sales</code> column. This is a fairly straight-forward calculation.</p> <p>We can also calculate the percentage change in sales using the <code>.pct_change()</code> method, which calculates the percentage change between the current and previous values. Under the hood, this method calculates the fractional change using the formula:</p> <pre><code>((value_current_row - value_previous_row) / value_previous_row)\n</code></pre> <p>So therefore we need to multiple the result by <code>100</code>.</p> Calculate day-over-day change<pre><code>daily_sales_pd[\"day_over_day_change\"] = daily_sales_pd[\"total_sales\"] - daily_sales_pd[\"previous_day_sales\"]\ndaily_sales_pd[\"pct_change\"] = daily_sales_pd[\"total_sales\"].pct_change() * 100\nprint(f\"Daily Sales with Day-over-Day Change: {len(daily_sales_pd)}\")\nprint(daily_sales_pd.head(5))\nprint(daily_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Daily Sales with Day-over-Day Change: 100\n</code></pre> <pre><code>         date  total_sales  previous_day_sales  next_day_sales  day_over_day_change  day_over_day_change  7d_moving_avg\n0  2023-01-01       490.76                 NaN          453.94                  NaN                  NaN     490.760000\n1  2023-01-02       453.94              490.76          994.51               -36.82               -36.82     472.350000\n2  2023-01-03       994.51              453.94          184.17               540.57               540.57     646.403333\n3  2023-01-04       184.17              994.51           27.89              -810.34              -810.34     530.845000\n4  2023-01-05        27.89              184.17          498.95              -156.28              -156.28     430.254000\n</code></pre> date total_sales previous_day_sales next_day_sales pct_change day_over_day_change 7d_moving_avg 0 2023-01-01 490.76 nan 453.94 nan nan 490.76 1 2023-01-02 453.94 490.76 994.51 -7.50265 -36.82 472.35 2 2023-01-03 994.51 453.94 184.17 119.084 540.57 646.403 3 2023-01-04 184.17 994.51 27.89 -81.4813 -810.34 530.845 4 2023-01-05 27.89 184.17 498.95 -84.8564 -156.28 430.254 <p>In SQL, we can calculate the day-over-day change in sales by subtracting the <code>previous_day_sales</code> column from the <code>total_sales</code> column. We can also calculate the percentage change in sales using the formula:</p> <pre><code>((current_day_sales - previous_day_sales) / previous_day_sales) * 100\n</code></pre> Day-over-day change already calculated<pre><code>dod_change_txt: str = \"\"\"\n    SELECT\n        sale_date,\n        total_sales,\n        previous_day_sales,\n        next_day_sales,\n        total_sales - previous_day_sales AS day_over_day_change,\n        ((total_sales - previous_day_sales) / previous_day_sales) * 100 AS pct_change\n    FROM (\n        SELECT\n            date AS sale_date,\n            SUM(sales_amount) AS total_sales,\n            LAG(SUM(sales_amount)) OVER (ORDER BY date) AS previous_day_sales,\n            LEAD(SUM(sales_amount)) OVER (ORDER BY date) AS next_day_sales\n        FROM sales\n        GROUP BY date\n    )\n    ORDER BY sale_date\n\"\"\"\ndod_change_df_sql: pd.DataFrame = pd.read_sql(dod_change_txt, conn)\nprint(f\"Daily Sales with Day-over-Day Change: {len(dod_change_df_sql)}\")\nprint(dod_change_df_sql.head(5))\nprint(dod_change_df_sql.head(5).to_markdown())\n</code></pre> <pre><code>Daily Sales with Day-over-Day Change: 100\n</code></pre> <pre><code>             sale_date  total_sales  previous_day_sales  next_day_sales  day_over_day_change  pct_change\n0  2023-01-01 00:00:00       490.76                 NaN          453.94                  NaN         NaN\n1  2023-01-02 00:00:00       453.94              490.76          994.51               -36.82   -7.502649\n2  2023-01-03 00:00:00       994.51              453.94          184.17               540.57  119.084020\n3  2023-01-04 00:00:00       184.17              994.51           27.89              -810.34  -81.481333\n4  2023-01-05 00:00:00        27.89              184.17          498.95              -156.28  -84.856383\n</code></pre> sale_date total_sales previous_day_sales next_day_sales day_over_day_change pct_change 0 2023-01-01 00:00:00 490.76 nan 453.94 nan nan 1 2023-01-02 00:00:00 453.94 490.76 994.51 -36.82 -7.50265 2 2023-01-03 00:00:00 994.51 453.94 184.17 540.57 119.084 3 2023-01-04 00:00:00 184.17 994.51 27.89 -810.34 -81.4813 4 2023-01-05 00:00:00 27.89 184.17 498.95 -156.28 -84.8564 <p>In PySpark, we can calculate the day-over-day change in sales by subtracting the <code>previous_day_sales</code> column from the <code>total_sales</code> column. We can also calculate the percentage change in sales using the formula:</p> <pre><code>((current_day_sales - previous_day_sales) / previous_day_sales) * 100\n</code></pre> <p>Here, we have again shown these calculations using two different methods: using the PySpark API and using the Spark SQL API. Realistically, the results for  both of them can be achieved using either method.</p> Calculate day-over-day change<pre><code>daily_sales_ps: psDataFrame = daily_sales_ps.withColumns(\n    {\n        \"day_over_day_change\": F.col(\"total_sales\") - F.col(\"previous_day_sales\"),\n        \"pct_change\": F.expr(\"((total_sales - previous_day_sales) / previous_day_sales) * 100\").alias(\"pct_change\"),\n    }\n)\nprint(f\"Daily Sales with Day-over-Day Change: {daily_sales_ps.count()}\")\ndaily_sales_ps.show(5)\nprint(daily_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Daily Sales with Day-over-Day Change: 100\n</code></pre> <pre><code>+----------+-----------+------------------+--------------+-------------------+------------------+\n|      date|total_sales|previous_day_sales|next_day_sales|day_over_day_change|        pct_change|\n+----------+-----------+------------------+--------------+-------------------+------------------+\n|2023-01-01|     490.76|              NULL|        453.94|               NULL|              NULL|\n|2023-01-02|     453.94|            490.76|        994.51| -36.81999999999999|-7.502648952644875|\n|2023-01-03|     994.51|            453.94|        184.17|  540.5699999999999|119.08401991452612|\n|2023-01-04|     184.17|            994.51|         27.89|            -810.34|-81.48133251551015|\n|2023-01-05|      27.89|            184.17|        498.95|-156.27999999999997|-84.85638268990606|\n+----------+-----------+------------------+--------------+-------------------+------------------+\nonly showing top 5 rows\n</code></pre> date total_sales previous_day_sales next_day_sales day_over_day_change pct_change 0 2023-01-01 490.76 nan 453.94 nan nan 1 2023-01-02 453.94 490.76 994.51 -36.82 -7.50265 2 2023-01-03 994.51 453.94 184.17 540.57 119.084 3 2023-01-04 184.17 994.51 27.89 -810.34 -81.4813 4 2023-01-05 27.89 184.17 498.95 -156.28 -84.8564 <p>In Polars, we can calculate the day-over-day change in sales by subtracting the <code>previous_day_sales</code> column from the <code>total_sales</code> column. We can also calculate the percentage change in sales using the formula:</p> <pre><code>((current_day_sales - previous_day_sales) / previous_day_sales) * 100\n</code></pre> Calculate day-over-day change<pre><code>daily_sales_pl: pl.DataFrame = daily_sales_pl.with_columns(\n    (pl.col(\"total_sales\") - pl.col(\"previous_day_sales\")).alias(\"day_over_day_change\"),\n    (pl.col(\"total_sales\") / pl.col(\"previous_day_sales\") - 1).alias(\"pct_change\") * 100,\n)\nprint(f\"Daily Sales with Day-over-Day Change: {len(daily_sales_pl)}\")\nprint(daily_sales_pl.head(5))\nprint(daily_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Daily Sales with Day-over-Day Change: 100\n</code></pre> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date       \u2506 total_sales \u2506 previous_day_sales \u2506 next_day_sales \u2506 day_over_day_change \u2506 pct_change \u2502\n\u2502 ---        \u2506 ---         \u2506 ---                \u2506 ---            \u2506 ---                 \u2506 ---        \u2502\n\u2502 date       \u2506 f64         \u2506 f64                \u2506 f64            \u2506 f64                 \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 \u2506 490.76      \u2506 null               \u2506 453.94         \u2506 null                \u2506 null       \u2502\n\u2502 2023-01-02 \u2506 453.94      \u2506 490.76             \u2506 994.51         \u2506 -36.82              \u2506 -7.502649  \u2502\n\u2502 2023-01-03 \u2506 994.51      \u2506 453.94             \u2506 184.17         \u2506 540.57              \u2506 119.08402  \u2502\n\u2502 2023-01-04 \u2506 184.17      \u2506 994.51             \u2506 27.89          \u2506 -810.34             \u2506 -81.481333 \u2502\n\u2502 2023-01-05 \u2506 27.89       \u2506 184.17             \u2506 498.95         \u2506 -156.28             \u2506 -84.856383 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date total_sales previous_day_sales next_day_sales day_over_day_change pct_change 0 2023-01-01 00:00:00 490.76 nan 453.94 nan nan 1 2023-01-02 00:00:00 453.94 490.76 994.51 -36.82 -7.50265 2 2023-01-03 00:00:00 994.51 453.94 184.17 540.57 119.084 3 2023-01-04 00:00:00 184.17 994.51 27.89 -810.34 -81.4813 4 2023-01-05 00:00:00 27.89 184.17 498.95 -156.28 -84.8564 <p>Next, we will calculate the rolling average of sales over a 7-day window. Rolling averages (aka moving averages) are useful for smoothing out short-term fluctuations and highlighting longer-term trends in the data. This is particularly useful in time series analysis, where we want to understand the underlying trend in the data without being overly influenced by short-term variations. It is also a very common technique used in financial analysis to analyze stock prices, sales data, and other time series data.</p> PandasSQLPySparkPolars <p>In Pandas, we can calculate the 7-day moving average of sales using the <code>.rolling()</code> method. This method allows us to specify a window size (in this case, <code>window=7</code> which is 7 days) and calculate the mean over that window. The <code>min_periods</code> parameter ensures that we get a value even if there are fewer than 7 days of data available at the start of the series. Finally, the <code>.mean()</code> method calculates the average over the specified window.</p> Calculate 7-day moving average<pre><code>daily_sales_pd[\"7d_moving_avg\"] = daily_sales_pd[\"total_sales\"].rolling(window=7, min_periods=1).mean()\nprint(f\"Daily Sales with 7-Day Moving Average: {len(daily_sales_pd)}\")\nprint(daily_sales_pd.head(5))\nprint(daily_sales_pd.head(5).to_markdown())\n</code></pre> <pre><code>Daily Sales with 7-Day Moving Average: 100\n</code></pre> <pre><code>         date  total_sales  previous_day_sales  next_day_sales  day_over_day_change  pct_change  7d_moving_avg\n0  2023-01-01       490.76                 NaN          453.94                  NaN         NaN     490.760000\n1  2023-01-02       453.94              490.76          994.51               -36.82   -7.502649     472.350000\n2  2023-01-03       994.51              453.94          184.17               540.57  119.084020     646.403333\n3  2023-01-04       184.17              994.51           27.89              -810.34  -81.481333     530.845000\n4  2023-01-05        27.89              184.17          498.95              -156.28  -84.856383     430.254000\n</code></pre> date total_sales previous_day_sales next_day_sales day_over_day_change pct_change 7d_moving_avg 0 2023-01-01 490.76 nan 453.94 nan nan 490.76 1 2023-01-02 453.94 490.76 994.51 -36.82 -7.50265 472.35 2 2023-01-03 994.51 453.94 184.17 540.57 119.084 646.403 3 2023-01-04 184.17 994.51 27.89 -810.34 -81.4813 530.845 4 2023-01-05 27.89 184.17 498.95 -156.28 -84.8564 430.254 <p>In SQL, we can calculate the 7-day moving average of sales using the <code>AVG()</code> window function with the <code>OVER</code> clause. It is important to include this <code>OVER</code> clause, because it is what the SQL engine uses to determine that it should be a Window function, rather than a regular aggregate function (which is specified using the <code>GROUP BY</code> clause).</p> <p>Here in our example, there are three different parts to the Window function:</p> <ol> <li>The <code>ORDER BY</code> clause: This specifies the order of the rows in the window. In this case, we are ordering by the <code>sale_date</code> column.</li> <li>The <code>ROWS BETWEEN</code> clause: This specifies the range of rows to include in the window. In this case, we are including the number of rows from 6 preceding rows to the current row. This means that for each row, the window will include the current row and the 6 rows before it, giving us a total of 7 rows in the window. It is important that you specify the <code>ORDER BY</code> clause before the <code>ROWS BETWEEN</code> clause to ensure that the correct rows are included in the window.</li> <li>The <code>AVG()</code> function: This calculates the average of the <code>total_sales</code> column over the specified window.</li> </ol> <p>Another peculiarity to note here is around the use of the sub-query. The sub-query is used to first calculate the daily sales, including the previous and next day sales, and the day-over-day change. This is because we need to calculate the moving average over the daily sales, rather than the individual sales transactions. The sub-query allows us to aggregate the sales data by date before calculating the moving average. The only change that we are including in the outer-query is the addition of the moving average calculation.</p> Calculate 7-day moving average<pre><code>rolling_avg_txt: str = \"\"\"\n    SELECT\n        sale_date,\n        total_sales,\n        previous_day_sales,\n        next_day_sales,\n        day_over_day_change,\n        pct_change,\n        AVG(total_sales) OVER (ORDER BY sale_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS \"7d_moving_avg\"\n    FROM (\n        SELECT\n            date AS sale_date,\n            SUM(sales_amount) AS total_sales,\n            LAG(SUM(sales_amount)) OVER (ORDER BY date) AS previous_day_sales,\n            LEAD(SUM(sales_amount)) OVER (ORDER BY date) AS next_day_sales,\n            SUM(sales_amount) - LAG(SUM(sales_amount)) OVER (ORDER BY date) AS day_over_day_change,\n            (SUM(sales_amount) / NULLIF(LAG(SUM(sales_amount)) OVER (ORDER BY date), 0) - 1) * 100 AS pct_change\n        FROM sales\n        GROUP BY date\n    ) AS daily_sales\n    ORDER BY sale_date\n\"\"\"\nwindow_df_sql: pd.DataFrame = pd.read_sql(rolling_avg_txt, conn)\nprint(f\"Daily Sales with 7-Day Moving Average: {len(window_df_sql)}\")\nprint(pd.read_sql(rolling_avg_txt + \" LIMIT 5\", conn))\nprint(pd.read_sql(rolling_avg_txt + \" LIMIT 5\", conn).to_markdown())\n</code></pre> <pre><code>Daily Sales with 7-Day Moving Average: 100\n</code></pre> <pre><code>             sale_date  total_sales  previous_day_sales  next_day_sales  day_over_day_change  pct_change  7d_moving_avg\n0  2023-01-01 00:00:00       490.76                 NaN          453.94                  NaN         NaN     490.760000\n1  2023-01-02 00:00:00       453.94              490.76          994.51               -36.82   -7.502649     472.350000\n2  2023-01-03 00:00:00       994.51              453.94          184.17               540.57  119.084020     646.403333\n3  2023-01-04 00:00:00       184.17              994.51           27.89              -810.34  -81.481333     530.845000\n4  2023-01-05 00:00:00        27.89              184.17          498.95              -156.28  -84.856383     430.254000\n</code></pre> sale_date total_sales previous_day_sales next_day_sales day_over_day_change pct_change 7d_moving_avg 0 2023-01-01 00:00:00 490.76 nan 453.94 nan nan 490.76 1 2023-01-02 00:00:00 453.94 490.76 994.51 -36.82 -7.50265 472.35 2 2023-01-03 00:00:00 994.51 453.94 184.17 540.57 119.084 646.403 3 2023-01-04 00:00:00 184.17 994.51 27.89 -810.34 -81.4813 530.845 4 2023-01-05 00:00:00 27.89 184.17 498.95 -156.28 -84.8564 430.254 <p>In PySpark, we can calculate the 7-day moving average of sales using the <code>F.avg()</code> function in combination with the <code>Window()</code> class. The <code>Window()</code> class allows us to define a window specification for the calculation. We can use the <code>.orderBy()</code> method to specify the order of the rows in the window, and the <code>.rowsBetween()</code> method to specify the range of rows to include in the window. The <code>F.avg()</code> function is then able to calculate the average of the <code>total_sales</code> column over the specified window.</p> <p>As with many aspects of PySpark, there are multiple ways to achieve the same result. In this case, we can use either the <code>F.avg()</code> function with the <code>Window()</code> class, or we can use the SQL expression syntax with the <code>F.expr()</code> function. Both methods will yield the same result.</p> Calculate 7-day moving average<pre><code>daily_sales_ps: psDataFrame = daily_sales_ps.withColumns(\n    {\n        \"7d_moving_avg\": F.avg(\"total_sales\").over(Window.orderBy(\"date\").rowsBetween(-6, 0)),\n        \"7d_rolling_avg\": F.expr(\"AVG(total_sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\"),\n    }\n)\nprint(f\"Daily Sales with 7-Day Moving Average: {daily_sales_ps.count()}\")\ndaily_sales_ps.show(5)\nprint(daily_sales_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Daily Sales with 7-Day Moving Average: 100\n</code></pre> <pre><code>+----------+-----------+------------------+--------------+-------------------+------------------+-----------------+-----------------+\n|      date|total_sales|previous_day_sales|next_day_sales|day_over_day_change|        pct_change|    7d_moving_avg|   7d_rolling_avg|\n+----------+-----------+------------------+--------------+-------------------+------------------+-----------------+-----------------+\n|2023-01-01|     490.76|              NULL|        453.94|               NULL|              NULL|           490.76|           490.76|\n|2023-01-02|     453.94|            490.76|        994.51| -36.81999999999999|-7.502648952644875|           472.35|           472.35|\n|2023-01-03|     994.51|            453.94|        184.17|  540.5699999999999|119.08401991452612|646.4033333333333|646.4033333333333|\n|2023-01-04|     184.17|            994.51|         27.89|            -810.34|-81.48133251551015|          530.845|          530.845|\n|2023-01-05|      27.89|            184.17|        498.95|-156.27999999999997|-84.85638268990606|          430.254|          430.254|\n+----------+-----------+------------------+--------------+-------------------+------------------+-----------------+-----------------+\nonly showing top 5 rows\n</code></pre> date total_sales previous_day_sales next_day_sales day_over_day_change pct_change 7d_moving_avg 7d_rolling_avg 0 2023-01-01 490.76 nan 453.94 nan nan 490.76 490.76 1 2023-01-02 453.94 490.76 994.51 -36.82 -7.50265 472.35 472.35 2 2023-01-03 994.51 453.94 184.17 540.57 119.084 646.403 646.403 3 2023-01-04 184.17 994.51 27.89 -810.34 -81.4813 530.845 530.845 4 2023-01-05 27.89 184.17 498.95 -156.28 -84.8564 430.254 430.254 <p>In Polars, we can calculate the 7-day moving average of sales using the <code>.rolling_mean()</code> method. This method allows us to specify a window size (in this case, <code>window_size=7</code> which is 7 days) and calculate the mean over that window. The <code>min_samples=1</code> parameter ensures that we get a value even if there are fewer than 7 days of data available at the start of the series.</p> Calculate 7-day moving average<pre><code>daily_sales_pl: pl.DataFrame = daily_sales_pl.with_columns(\n    pl.col(\"total_sales\").rolling_mean(window_size=7, min_samples=1).alias(\"7d_moving_avg\"),\n)\nprint(f\"Daily Sales with 7-Day Moving Average: {len(daily_sales_pl)}\")\nprint(daily_sales_pl.head(5))\nprint(daily_sales_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Daily Sales with 7-Day Moving Average: 100\n</code></pre> <pre><code>shape: (5, 7)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date       \u2506 total_sales \u2506 previous_day_sales \u2506 next_day_sales \u2506 day_over_day_change \u2506 pct_change \u2506 7d_moving_avg \u2502\n\u2502 ---        \u2506 ---         \u2506 ---                \u2506 ---            \u2506 ---                 \u2506 ---        \u2506 ---           \u2502\n\u2502 date       \u2506 f64         \u2506 f64                \u2506 f64            \u2506 f64                 \u2506 f64        \u2506 f64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2023-01-01 \u2506 490.76      \u2506 null               \u2506 453.94         \u2506 null                \u2506 null       \u2506 490.76        \u2502\n\u2502 2023-01-02 \u2506 453.94      \u2506 490.76             \u2506 994.51         \u2506 -36.82              \u2506 -7.502649  \u2506 472.35        \u2502\n\u2502 2023-01-03 \u2506 994.51      \u2506 453.94             \u2506 184.17         \u2506 540.57              \u2506 119.08402  \u2506 646.403333    \u2502\n\u2502 2023-01-04 \u2506 184.17      \u2506 994.51             \u2506 27.89          \u2506 -810.34             \u2506 -81.481333 \u2506 530.845       \u2502\n\u2502 2023-01-05 \u2506 27.89       \u2506 184.17             \u2506 498.95         \u2506 -156.28             \u2506 -84.856383 \u2506 430.254       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> date total_sales previous_day_sales next_day_sales day_over_day_change pct_change 7d_moving_avg 0 2023-01-01 00:00:00 490.76 nan 453.94 nan nan 490.76 1 2023-01-02 00:00:00 453.94 490.76 994.51 -36.82 -7.50265 472.35 2 2023-01-03 00:00:00 994.51 453.94 184.17 540.57 119.084 646.403 3 2023-01-04 00:00:00 184.17 994.51 27.89 -810.34 -81.4813 530.845 4 2023-01-05 00:00:00 27.89 184.17 498.95 -156.28 -84.8564 430.254 <p>Finally, we can visualize the daily sales data along with the 7-day moving average using Plotly. This allows us to see the trends in sales over time and how the moving average smooths out the fluctuations in daily sales.</p> <p>For this, we will again utilise Plotly to create an interactive line chart that displays both the daily sales and the 7-day moving average. The chart will have the date on the x-axis and the sales amount on the y-axis, with two lines representing the daily sales and the moving average.</p> <p>The graph will be instantiated using the <code>go.Figure()</code> class, and using the <code>.add_trace()</code> method we will add two traces to the figure: one for the daily sales and one for the 7-day moving average. The <code>go.Scatter()</code> class is used to create the line traces, by defining <code>mode=\"lines\"</code> to display the data as a line chart.</p> <p>Finally, we will use the <code>.update_layout()</code> method to set the titles for the chart, and the position of the legend.</p> PandasSQLPySparkPolars <p>Plotly is easily able to handle Pandas DataFrames, so we can directly parse the columns from the DataFrame to create the traces for the daily sales and the 7-day moving average.</p> Plot results<pre><code>fig: go.Figure = (\n    go.Figure()\n    .add_trace(\n        go.Scatter(\n            x=daily_sales_pd[\"date\"],\n            y=daily_sales_pd[\"total_sales\"],\n            mode=\"lines\",\n            name=\"Daily Sales\",\n        )\n    )\n    .add_trace(\n        go.Scatter(\n            x=daily_sales_pd[\"date\"],\n            y=daily_sales_pd[\"7d_moving_avg\"],\n            mode=\"lines\",\n            name=\"7-Day Moving Average\",\n            line_width=3,\n        ),\n    )\n    .update_layout(\n        title=\"Daily Sales with 7-Day Moving Average\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Sales Amount ($)\",\n        legend_orientation=\"h\",\n        legend_yanchor=\"bottom\",\n        legend_y=1,\n    )\n)\nfig.write_html(\"images/pt4_daily_sales_with_7d_avg_pd.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p> <p>Plotly is easily able to handle Pandas DataFrames, so we can directly parse the columns from the DataFrame to create the traces for the daily sales and the 7-day moving average.</p> Plot results<pre><code>fig: go.Figure = (\n    go.Figure()\n    .add_trace(\n        go.Scatter(\n            x=window_df_sql[\"sale_date\"],\n            y=window_df_sql[\"total_sales\"],\n            mode=\"lines\",\n            name=\"Daily Sales\",\n        )\n    )\n    .add_trace(\n        go.Scatter(\n            x=window_df_sql[\"sale_date\"],\n            y=window_df_sql[\"7d_moving_avg\"],\n            mode=\"lines\",\n            name=\"7-Day Moving Average\",\n            line_width=3,\n        )\n    )\n    .update_layout(\n        title=\"Daily Sales with 7-Day Moving Average\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Sales Amount ($)\",\n        legend_orientation=\"h\",\n        legend_yanchor=\"bottom\",\n        legend_y=1,\n    )\n)\nfig.write_html(\"images/pt4_daily_sales_with_7d_avg_sql.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p> <p>Plotly is not able to interpret PySpark DataFrames directly, so we need to convert the PySpark DataFrame to a Pandas DataFrame before plotting. This can be done using the <code>.toPandas()</code> method. We can then parse the columns from the Pandas DataFrame to create the traces for the daily sales and the 7-day moving average.</p> Plot results<pre><code>fig: go.Figure = (\n    go.Figure()\n    .add_trace(\n        go.Scatter(\n            x=daily_sales_ps.toPandas()[\"date\"],\n            y=daily_sales_ps.toPandas()[\"total_sales\"],\n            mode=\"lines\",\n            name=\"Daily Sales\",\n        )\n    )\n    .add_trace(\n        go.Scatter(\n            x=daily_sales_ps.toPandas()[\"date\"],\n            y=daily_sales_ps.toPandas()[\"7d_moving_avg\"],\n            mode=\"lines\",\n            name=\"7-Day Moving Average\",\n            line_width=3,\n        ),\n    )\n    .update_layout(\n        title=\"Daily Sales with 7-Day Moving Average\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Sales Amount ($)\",\n        legend_orientation=\"h\",\n        legend_yanchor=\"bottom\",\n        legend_y=1,\n    )\n)\nfig.write_html(\"images/pt4_daily_sales_with_7d_avg_ps.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p> <p>Plotly is easily able to handle Polars DataFrames, so we can directly parse the columns from the DataFrame to create the traces for the daily sales and the 7-day moving average.</p> Plot results<pre><code>fig: go.Figure = (\n    go.Figure()\n    .add_trace(\n        go.Scatter(\n            x=daily_sales_pl[\"date\"],\n            y=daily_sales_pl[\"total_sales\"],\n            mode=\"lines\",\n            name=\"Daily Sales\",\n        )\n    )\n    .add_trace(\n        go.Scatter(\n            x=daily_sales_pl[\"date\"],\n            y=daily_sales_pl[\"7d_moving_avg\"],\n            mode=\"lines\",\n            name=\"7-Day Moving Average\",\n            line_width=3,\n        )\n    )\n    .update_layout(\n        title=\"Daily Sales with 7-Day Moving Average\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"Sales Amount ($)\",\n        legend_orientation=\"h\",\n        legend_yanchor=\"bottom\",\n        legend_y=1,\n    )\n)\nfig.write_html(\"images/pt4_daily_sales_with_7d_avg_pl.html\", include_plotlyjs=\"cdn\", full_html=True)\nfig.show()\n</code></pre> <p> </p>"},{"location":"guides/querying-data/#5-ranking-and-partitioning","title":"5. Ranking and Partitioning","text":"<p>The fifth section will demonstrate how to rank and partition data. This is useful for identifying top performers, such as the highest spending customers or the most popular products.</p> PandasSQLPySparkPolars <p>In Pandas, we can use the <code>.rank()</code> method to rank values in a DataFrame. This method allows us to specify the ranking method (e.g., dense, average, min, max) and whether to rank in ascending or descending order.</p> Rank customers by total spending<pre><code>customer_spending_pd: pd.DataFrame = df_sales_pd.groupby(\"customer_id\").agg(total_spending=(\"sales_amount\", \"sum\"))\ncustomer_spending_pd[\"rank\"] = customer_spending_pd[\"total_spending\"].rank(method=\"dense\", ascending=False)\ncustomer_spending_pd: pd.DataFrame = customer_spending_pd.sort_values(\"rank\").reset_index()\nprint(f\"Customer Spending Summary: {len(customer_spending_pd)}\")\nprint(customer_spending_pd.head(5))\nprint(customer_spending_pd.head(5).to_markdown())\n</code></pre> <pre><code>Customer Spending Summary: 61\n</code></pre> <pre><code>   customer_id  total_spending  rank\n0           15         2297.55   1.0\n1            4         2237.49   2.0\n2           62         2177.35   3.0\n3           60         2086.09   4.0\n4           21         2016.95   5.0\n</code></pre> customer_id total_spending rank 0 15 2297.55 1 1 4 2237.49 2 2 62 2177.35 3 3 60 2086.09 4 4 21 2016.95 5 <p>In SQL, we can use the <code>DENSE_RANK()</code> window function to rank values in a query. This function assigns a rank to each row within a partition of a given result set, with no gaps in the ranking values. Note that this function can only be used in congunction with the <code>OVER</code> clause, which defines it as a Window function. The <code>ORDER BY</code> clause within the <code>OVER</code> clause specifies the order in which the rows are ranked.</p> Rank customers by total spending<pre><code>customer_spending_txt: str = \"\"\"\n    SELECT\n        customer_id,\n        SUM(sales_amount) AS total_spending,\n        DENSE_RANK() OVER (ORDER BY SUM(sales_amount) DESC) AS rank\n    FROM sales\n    GROUP BY customer_id\n    ORDER BY rank\n\"\"\"\ncustomer_spending_sql: pd.DataFrame = pd.read_sql(customer_spending_txt, conn)\nprint(f\"Customer Spending Summary: {len(customer_spending_sql)}\")\nprint(customer_spending_sql.head(5))\nprint(customer_spending_sql.head(5).to_markdown())\n</code></pre> <pre><code>Customer Spending Summary: 61\n</code></pre> <pre><code>   customer_id  total_spending  rank\n0           15         2297.55     1\n1            4         2237.49     2\n2           62         2177.35     3\n3           60         2086.09     4\n4           21         2016.95     5\n</code></pre> customer_id total_spending rank 0 15 2297.55 1 1 4 2237.49 2 2 62 2177.35 3 3 60 2086.09 4 4 21 2016.95 5 <p>In PySpark, we can use the <code>F.dense_rank()</code> function in combination with the <code>Window()</code> class to rank values in a DataFrame. The <code>Window()</code> class allows us to define a window specification for the calculation, and the <code>F.dense_rank()</code> function calculates the dense rank of each row within that window.</p> Rank customers by total spending<pre><code>customer_spending_ps: psDataFrame = (\n    df_sales_ps.groupBy(\"customer_id\")\n    .agg(F.sum(\"sales_amount\").alias(\"total_spending\"))\n    .withColumn(\"rank\", F.dense_rank().over(Window.orderBy(F.desc(\"total_spending\"))))\n    .orderBy(\"rank\")\n)\nprint(f\"Customer Spending Summary: {customer_spending_ps.count()}\")\ncustomer_spending_ps.show(5)\nprint(customer_spending_ps.limit(5).toPandas().to_markdown())\n</code></pre> <pre><code>Customer Spending Summary: 61\n</code></pre> <pre><code>+-----------+------------------+----+\n|customer_id|    total_spending|rank|\n+-----------+------------------+----+\n|         15|           2297.55|   1|\n|          4|           2237.49|   2|\n|         62|           2177.35|   3|\n|         60|2086.0899999999997|   4|\n|         21|           2016.95|   5|\n+-----------+------------------+----+\n</code></pre> customer_id total_spending rank 0 15 2297.55 1 1 4 2237.49 2 2 62 2177.35 3 3 60 2086.09 4 4 21 2016.95 5 <p>In Polars, we can use the <code>.rank()</code> method to rank values in a DataFrame. This method allows us to specify the ranking method (e.g., dense, average, min, max) and whether to rank in ascending or descending order.</p> Rank customers by total spending<pre><code>customer_spending_pl: pl.DataFrame = (\n    df_sales_pl.group_by(\"customer_id\")\n    .agg(pl.col(\"sales_amount\").sum().alias(\"total_spending\"))\n    .with_columns(\n        pl.col(\"total_spending\").rank(method=\"dense\", descending=True).alias(\"rank\"),\n    )\n    .sort(\"rank\")\n)\nprint(f\"Customer Spending Summary: {len(customer_spending_pl)}\")\nprint(customer_spending_pl.head(5))\nprint(customer_spending_pl.head(5).to_pandas().to_markdown())\n</code></pre> <pre><code>Customer Spending Summary: 61\n</code></pre> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 total_spending \u2506 rank \u2502\n\u2502 ---         \u2506 ---            \u2506 ---  \u2502\n\u2502 i64         \u2506 f64            \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 15          \u2506 2297.55        \u2506 1    \u2502\n\u2502 4           \u2506 2237.49        \u2506 2    \u2502\n\u2502 62          \u2506 2177.35        \u2506 3    \u2502\n\u2502 60          \u2506 2086.09        \u2506 4    \u2502\n\u2502 21          \u2506 2016.95        \u2506 5    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> customer_id total_spending rank 0 15 2297.55 1 1 4 2237.49 2 2 62 2177.35 3 3 60 2086.09 4 4 21 2016.95 5 <p>Next, we will rank products based on the quantity sold, partitioned by the product category. This will help us identify the most popular products within each category.</p> PandasSQLPySparkPolars <p>In Pandas it is first necessary to group the sales data by <code>category</code> and <code>product_id</code>, then aggregate the data for the <code>.sum()</code> of the <code>quantity</code> to find the <code>total_quantity</code> sold for each product. After that, we can use the <code>.rank()</code> method to rank the products within each category based on the total quantity sold.</p> <p>It is important to note here that we are implementing the <code>.rank()</code> method from within an <code>.assign()</code> method. This is a common pattern in Pandas to create new columns on a DataFrame based on other columns already existing on the DataFrame, while keeping the DataFrame immutable. Here, we are using the <code>.groupby()</code> method to group the DataFrame by <code>category</code>, and then applying the <code>.rank()</code> method to the <code>total_quantity</code> column within each category. In this way, we are creating a partitioned DataFrame that ranks products by quantity sold within each category.</p> Rank products by quantity sold, by category<pre><code>product_popularity_pd: pd.DataFrame = (\n    df_sales_pd.groupby([\"category\", \"product_id\"])\n    .agg(\n        total_quantity=(\"quantity\", \"sum\"),\n    )\n    .reset_index()\n    .assign(\n        rank=lambda x: x.groupby(\"category\")[\"total_quantity\"].rank(method=\"dense\", ascending=False),\n    )\n    .sort_values([\"rank\", \"category\"])\n    .reset_index(drop=True)\n)\nprint(f\"Product Popularity Summary: {len(product_popularity_pd)}\")\nprint(product_popularity_pd.head(10))\nprint(product_popularity_pd.head(10).to_markdown())\n</code></pre> <pre><code>Product Popularity Summary: 78\n</code></pre> <pre><code>      category  product_id  total_quantity  rank\n0        Books          11              14   1.0\n1     Clothing           7               9   1.0\n2  Electronics          37              16   1.0\n3         Food          45              34   1.0\n4         Home           3              10   1.0\n5        Books          28               9   2.0\n6     Clothing          35               8   2.0\n7  Electronics          35              11   2.0\n8         Food           1              16   2.0\n9         Home           9               5   2.0\n</code></pre> category product_id total_quantity rank 0 Books 11 14 1 1 Clothing 7 9 1 2 Electronics 37 16 1 3 Food 45 34 1 4 Home 3 10 1 5 Books 28 9 2 6 Clothing 35 8 2 7 Electronics 35 11 2 8 Food 1 16 2 9 Home 9 5 2 <p>In SQL, we can use the <code>RANK()</code> window function to rank products within each category based on the total quantity sold. The <code>PARTITION BY</code> clause allows us to partition the data by <code>category</code>, and the <code>ORDER BY</code> clause specifies the order in which the rows are ranked within each partition.</p> Rank products by quantity sold, by category<pre><code>product_popularity_txt: str = \"\"\"\n    SELECT\n        category,\n        product_id,\n        SUM(quantity) AS total_quantity,\n        RANK() OVER (PARTITION BY category ORDER BY SUM(quantity) DESC) AS rank\n    FROM sales\n    GROUP BY category, product_id\n    ORDER BY rank\n\"\"\"\nprint(f\"Product Popularity: {len(pd.read_sql(product_popularity_txt, conn))}\")\nprint(pd.read_sql(product_popularity_txt + \"LIMIT 10\", conn))\nprint(pd.read_sql(product_popularity_txt + \"LIMIT 10\", conn).to_markdown())\n</code></pre> <pre><code>Product Popularity: 78\n</code></pre> <pre><code>      category  product_id  total_quantity  rank\n0        Books          11              14     1\n1     Clothing           7               9     1\n2  Electronics          37              16     1\n3         Food          45              34     1\n4         Home           3              10     1\n5        Books          28               9     2\n6     Clothing          35               8     2\n7  Electronics          35              11     2\n8         Food           1              16     2\n9         Home          48               5     2\n</code></pre> category product_id total_quantity rank 0 Books 11 14 1 1 Clothing 7 9 1 2 Electronics 37 16 1 3 Food 45 34 1 4 Home 3 10 1 5 Books 28 9 2 6 Clothing 35 8 2 7 Electronics 35 11 2 8 Food 1 16 2 9 Home 48 5 2 <p>In PySpark, we can use the <code>F.dense_rank()</code> function in combination with the <code>Window()</code> class to rank products within each category based on the total quantity sold. We can define the partitioning by using the <code>.partitionBy()</code> method and parse'ing in the <code>\"category\"</code> column. We can then define the ordering by using the <code>.orderBy()</code> method and parse'ing in the <code>\"total_quantity\"</code> expression to order the products by total quantity sold in descending order with the <code>F.desc()</code> method.</p> <p>Here, we have also provided an alternative way to define the rank by using the Spark SQL method. The outcome is the same, it's simply written in a SQL-like expression.</p> Rank products by quantity sold, by category<pre><code>product_popularity_ps: psDataFrame = (\n    df_sales_ps.groupBy(\"category\", \"product_id\")\n    .agg(F.sum(\"quantity\").alias(\"total_quantity\"))\n    .withColumns(\n        {\n            \"rank_p\": F.dense_rank().over(Window.partitionBy(\"category\").orderBy(F.desc(\"total_quantity\"))),\n            \"rank_s\": F.expr(\"DENSE_RANK() OVER (PARTITION BY category ORDER BY total_quantity DESC)\"),\n        }\n    )\n    .orderBy(\"rank_p\")\n)\nprint(f\"Product Popularity Summary: {product_popularity_ps.count()}\")\nproduct_popularity_ps.show(10)\nprint(product_popularity_ps.limit(10).toPandas().to_markdown())\n</code></pre> <pre><code>Product Popularity Summary: 78\n</code></pre> <pre><code>+-----------+----------+--------------+------+------+\n|   category|product_id|total_quantity|rank_p|rank_s|\n+-----------+----------+--------------+------+------+\n|   Clothing|         7|             9|     1|     1|\n|      Books|        11|            14|     1|     1|\n|Electronics|        37|            16|     1|     1|\n|       Food|        45|            34|     1|     1|\n|       Home|         3|            10|     1|     1|\n|      Books|        28|             9|     2|     2|\n|Electronics|        35|            11|     2|     2|\n|       Home|        29|             5|     2|     2|\n|       Home|        48|             5|     2|     2|\n|       Home|         9|             5|     2|     2|\n+-----------+----------+--------------+------+------+\nonly showing top 10 rows\n</code></pre> category product_id total_quantity rank_p rank_s 0 Clothing 7 9 1 1 1 Books 11 14 1 1 2 Electronics 37 16 1 1 3 Food 45 34 1 1 4 Home 3 10 1 1 5 Books 28 9 2 2 6 Clothing 35 8 2 2 7 Electronics 35 11 2 2 8 Food 1 16 2 2 9 Home 29 5 2 2 <p>In Polars, we can use the <code>.rank()</code> method to rank products within each category based on the total quantity sold. We first group the sales data by <code>category</code> and <code>product_id</code>, then aggregate the data for the <code>.sum()</code> of the <code>quantity</code> to find the <code>total_quantity</code> sold for each product. After that, we can use the <code>.rank()</code> method to rank the products within each category based on the total quantity sold. Finally, we can define the partitioning by using the <code>.over()</code> method and parse'ing in <code>partition_by=\"category\"</code>.</p> Rank products by quantity sold, by category<pre><code>product_popularity_pl: pl.DataFrame = (\n    df_sales_pl.group_by(\"category\", \"product_id\")\n    .agg(pl.sum(\"quantity\").alias(\"total_quantity\"))\n    .with_columns(\n        pl.col(\"total_quantity\").rank(method=\"dense\", descending=True).over(partition_by=\"category\").alias(\"rank\")\n    )\n    .sort(\"rank\", \"category\")\n)\nprint(f\"Product Popularity Summary: {len(product_popularity_pl)}\")\nprint(product_popularity_pl.head(10))\nprint(product_popularity_pl.head(10).to_pandas().to_markdown())\n</code></pre> <pre><code>Product Popularity Summary: 78\n</code></pre> <pre><code>shape: (10, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 category    \u2506 product_id \u2506 total_quantity \u2506 rank \u2502\n\u2502 ---         \u2506 ---        \u2506 ---            \u2506 ---  \u2502\n\u2502 str         \u2506 i64        \u2506 i64            \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Books       \u2506 11         \u2506 14             \u2506 1    \u2502\n\u2502 Clothing    \u2506 7          \u2506 9              \u2506 1    \u2502\n\u2502 Electronics \u2506 37         \u2506 16             \u2506 1    \u2502\n\u2502 Food        \u2506 45         \u2506 34             \u2506 1    \u2502\n\u2502 Home        \u2506 3          \u2506 10             \u2506 1    \u2502\n\u2502 Books       \u2506 28         \u2506 9              \u2506 2    \u2502\n\u2502 Clothing    \u2506 35         \u2506 8              \u2506 2    \u2502\n\u2502 Electronics \u2506 35         \u2506 11             \u2506 2    \u2502\n\u2502 Food        \u2506 1          \u2506 16             \u2506 2    \u2502\n\u2502 Home        \u2506 48         \u2506 5              \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> category product_id total_quantity rank 0 Books 11 14 1 1 Clothing 7 9 1 2 Electronics 37 16 1 3 Food 45 34 1 4 Home 3 10 1 5 Books 28 9 2 6 Clothing 35 8 2 7 Electronics 35 11 2 8 Food 1 16 2 9 Home 48 5 2"},{"location":"guides/querying-data/#conclusion","title":"Conclusion","text":"<p>This comprehensive guide has demonstrated how to perform essential data querying and manipulation operations across four powerful tools: Pandas, SQL, PySpark, and Polars. Each tool brings unique advantages to the data processing landscape, and understanding their strengths helps you choose the right tool for your specific use case.</p>"},{"location":"guides/querying-data/#tool-comparison-and-use-cases","title":"Tool Comparison and Use Cases","text":"<ul> <li> <p>Pandas has an extensive ecosystem, making it ideal for:</p> <ul> <li>Small to medium datasets (up to millions of rows)</li> <li>Interactive data exploration and visualization</li> <li>Data preprocessing for machine learning workflows</li> <li>Quick statistical analysis and reporting</li> </ul> <p>Pandas remains the go-to choice for exploratory data analysis and rapid prototyping.</p> </li> <li> <p>SQL excels in:</p> <ul> <li>Working with relational databases and data warehouses</li> <li>Complex joins and subqueries</li> <li>Declarative data transformations</li> <li>Team environments where SQL knowledge is widespread</li> </ul> <p>SQL provides the universal language of data with unmatched expressiveness for complex queries</p> </li> <li> <p>PySpark is great for when you need:</p> <ul> <li>Processing datasets that don't fit in memory (terabytes or larger)</li> <li>Distributed computing across clusters</li> <li>Integration with Hadoop ecosystem components</li> <li>Scalable machine learning with MLlib</li> </ul> <p>PySpark unlocks the power of distributed computing for big data scenarios.</p> </li> <li> <p>Polars is particularly valuable for:</p> <ul> <li>Large datasets that require fast processing (gigabytes to small terabytes)</li> <li>Performance-critical applications</li> <li>Memory-constrained environments</li> <li>Lazy evaluation and query optimization</li> </ul> <p>Polars emerges as the high-performance alternative with excellent memory efficiency.</p> </li> </ul>"},{"location":"guides/querying-data/#key-techniques-covered","title":"Key Techniques Covered","text":"<p>Throughout this guide, we've explored fundamental data manipulation patterns that remain consistent across all tools:</p> <ol> <li>Data Filtering and Selection - Essential for subsetting data based on conditions</li> <li>Grouping and Aggregation - Critical for summarizing data by categories</li> <li>Joining and Merging - Necessary for combining data from multiple sources</li> <li>Window Functions - Powerful for time-series analysis and advanced calculations</li> <li>Ranking and Partitioning - Useful for identifying top performers and comparative analysis</li> </ol>"},{"location":"guides/querying-data/#best-practices-and-recommendations","title":"Best Practices and Recommendations","text":"<p>When working with any of these tools, consider these best practices:</p> <ul> <li>Start with the right tool: Match your tool choice to your data size, infrastructure, and team expertise</li> <li>Understand your data: Always examine data types, null values, and distributions before processing</li> <li>Optimize for readability: Write clear, well-documented code that your future self and teammates can understand</li> <li>Profile performance: Measure execution time and memory usage, especially for large datasets</li> <li>Leverage built-in optimizations: Use vectorized operations, avoid loops, and take advantage of lazy evaluation where available</li> </ul>"},{"location":"guides/querying-data/#moving-forward","title":"Moving Forward","text":"<p>The data landscape continues to evolve rapidly, with new tools and techniques emerging regularly. The fundamental concepts demonstrated in this guide\u2014filtering, grouping, joining, and analytical functions\u2014remain constant across platforms. By mastering these core concepts, you'll be well-equipped to adapt to new tools and technologies as they arise.</p> <p>Whether you're analyzing customer behavior, processing sensor data, or building machine learning models, the techniques in this guide provide a solid foundation for effective data manipulation. Remember that the best tool is often the one that best fits your specific requirements for performance, scalability, and team capabilities.</p> <p>Continue practicing with real datasets, explore advanced features of each tool, and stay curious about emerging technologies in the data processing ecosystem. The skills you've learned here will serve as building blocks for increasingly sophisticated data analysis and engineering tasks.</p>"},{"location":"toolboxes/docstring-format-checker/","title":"Synthetic Data Generators","text":""},{"location":"toolboxes/synthetic-data-generators/","title":"Synthetic Data Generators","text":""},{"location":"toolboxes/toolbox-pyspark/","title":"PySpark Toolbox","text":""},{"location":"toolboxes/toolbox-python/","title":"Python Toolbox","text":""},{"location":"toolboxes/ts-stat-tests/","title":"Time Series Statistical Tests","text":""}]}